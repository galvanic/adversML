{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From pipeline to pandas\n",
    "\n",
    "Get experimental results in a neat pandas DataFrame with performance metrics as columns and experimental specifications as rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 1,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.905\n",
      "iteration 3:\tcost = 0.834\n",
      "iteration 4:\tcost = 0.781\n",
      "iteration 5:\tcost = 0.740\n",
      "iteration 6:\tcost = 0.708\n",
      "iteration 7:\tcost = 0.682\n",
      "iteration 8:\tcost = 0.660\n",
      "iteration 9:\tcost = 0.641\n",
      "iteration 10:\tcost = 0.625\n",
      "iteration 11:\tcost = 0.611\n",
      "iteration 12:\tcost = 0.598\n",
      "iteration 13:\tcost = 0.587\n",
      "iteration 14:\tcost = 0.576\n",
      "iteration 15:\tcost = 0.567\n",
      "iteration 16:\tcost = 0.558\n",
      "iteration 17:\tcost = 0.549\n",
      "iteration 18:\tcost = 0.542\n",
      "iteration 19:\tcost = 0.535\n",
      "iteration 20:\tcost = 0.528\n",
      "{'AUC': 0.625,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.75,\n",
      " 'error_test': 0.14999999999999999,\n",
      " 'error_train': 0.17799999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 2,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.891\n",
      "iteration 3:\tcost = 0.812\n",
      "iteration 4:\tcost = 0.754\n",
      "iteration 5:\tcost = 0.711\n",
      "iteration 6:\tcost = 0.678\n",
      "iteration 7:\tcost = 0.652\n",
      "iteration 8:\tcost = 0.631\n",
      "iteration 9:\tcost = 0.614\n",
      "iteration 10:\tcost = 0.599\n",
      "iteration 11:\tcost = 0.587\n",
      "iteration 12:\tcost = 0.576\n",
      "iteration 13:\tcost = 0.566\n",
      "iteration 14:\tcost = 0.557\n",
      "iteration 15:\tcost = 0.549\n",
      "iteration 16:\tcost = 0.541\n",
      "iteration 17:\tcost = 0.534\n",
      "iteration 18:\tcost = 0.528\n",
      "iteration 19:\tcost = 0.522\n",
      "iteration 20:\tcost = 0.516\n",
      "{'AUC': 0.56363636363636371,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.87272727272727268,\n",
      " 'error_test': 0.192,\n",
      " 'error_train': 0.17000000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 3,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.871\n",
      "iteration 3:\tcost = 0.779\n",
      "iteration 4:\tcost = 0.712\n",
      "iteration 5:\tcost = 0.663\n",
      "iteration 6:\tcost = 0.626\n",
      "iteration 7:\tcost = 0.598\n",
      "iteration 8:\tcost = 0.576\n",
      "iteration 9:\tcost = 0.558\n",
      "iteration 10:\tcost = 0.543\n",
      "iteration 11:\tcost = 0.530\n",
      "iteration 12:\tcost = 0.519\n",
      "iteration 13:\tcost = 0.510\n",
      "iteration 14:\tcost = 0.501\n",
      "iteration 15:\tcost = 0.493\n",
      "iteration 16:\tcost = 0.486\n",
      "iteration 17:\tcost = 0.479\n",
      "iteration 18:\tcost = 0.473\n",
      "iteration 19:\tcost = 0.467\n",
      "iteration 20:\tcost = 0.461\n",
      "{'AUC': 0.55600000000000005,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.88800000000000001,\n",
      " 'error_test': 0.222,\n",
      " 'error_train': 0.16200000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 4,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.894\n",
      "iteration 3:\tcost = 0.818\n",
      "iteration 4:\tcost = 0.762\n",
      "iteration 5:\tcost = 0.720\n",
      "iteration 6:\tcost = 0.688\n",
      "iteration 7:\tcost = 0.662\n",
      "iteration 8:\tcost = 0.642\n",
      "iteration 9:\tcost = 0.624\n",
      "iteration 10:\tcost = 0.610\n",
      "iteration 11:\tcost = 0.597\n",
      "iteration 12:\tcost = 0.586\n",
      "iteration 13:\tcost = 0.575\n",
      "iteration 14:\tcost = 0.566\n",
      "iteration 15:\tcost = 0.558\n",
      "iteration 16:\tcost = 0.550\n",
      "iteration 17:\tcost = 0.543\n",
      "iteration 18:\tcost = 0.536\n",
      "iteration 19:\tcost = 0.530\n",
      "iteration 20:\tcost = 0.524\n",
      "{'AUC': 0.59174311926605505,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.8165137614678899,\n",
      " 'error_test': 0.17799999999999999,\n",
      " 'error_train': 0.17599999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 5,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.887\n",
      "iteration 3:\tcost = 0.807\n",
      "iteration 4:\tcost = 0.748\n",
      "iteration 5:\tcost = 0.704\n",
      "iteration 6:\tcost = 0.670\n",
      "iteration 7:\tcost = 0.644\n",
      "iteration 8:\tcost = 0.623\n",
      "iteration 9:\tcost = 0.605\n",
      "iteration 10:\tcost = 0.591\n",
      "iteration 11:\tcost = 0.578\n",
      "iteration 12:\tcost = 0.567\n",
      "iteration 13:\tcost = 0.557\n",
      "iteration 14:\tcost = 0.548\n",
      "iteration 15:\tcost = 0.539\n",
      "iteration 16:\tcost = 0.532\n",
      "iteration 17:\tcost = 0.525\n",
      "iteration 18:\tcost = 0.518\n",
      "iteration 19:\tcost = 0.512\n",
      "iteration 20:\tcost = 0.506\n",
      "{'AUC': 0.56956521739130439,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.86086956521739133,\n",
      " 'error_test': 0.19800000000000001,\n",
      " 'error_train': 0.17599999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 6,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.895\n",
      "iteration 3:\tcost = 0.818\n",
      "iteration 4:\tcost = 0.761\n",
      "iteration 5:\tcost = 0.718\n",
      "iteration 6:\tcost = 0.685\n",
      "iteration 7:\tcost = 0.658\n",
      "iteration 8:\tcost = 0.636\n",
      "iteration 9:\tcost = 0.618\n",
      "iteration 10:\tcost = 0.602\n",
      "iteration 11:\tcost = 0.588\n",
      "iteration 12:\tcost = 0.576\n",
      "iteration 13:\tcost = 0.565\n",
      "iteration 14:\tcost = 0.555\n",
      "iteration 15:\tcost = 0.546\n",
      "iteration 16:\tcost = 0.537\n",
      "iteration 17:\tcost = 0.530\n",
      "iteration 18:\tcost = 0.522\n",
      "iteration 19:\tcost = 0.516\n",
      "iteration 20:\tcost = 0.509\n",
      "{'AUC': 0.60280373831775702,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.79439252336448596,\n",
      " 'error_test': 0.17000000000000001,\n",
      " 'error_train': 0.17000000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 7,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.899\n",
      "iteration 3:\tcost = 0.826\n",
      "iteration 4:\tcost = 0.772\n",
      "iteration 5:\tcost = 0.731\n",
      "iteration 6:\tcost = 0.700\n",
      "iteration 7:\tcost = 0.676\n",
      "iteration 8:\tcost = 0.656\n",
      "iteration 9:\tcost = 0.640\n",
      "iteration 10:\tcost = 0.625\n",
      "iteration 11:\tcost = 0.613\n",
      "iteration 12:\tcost = 0.602\n",
      "iteration 13:\tcost = 0.593\n",
      "iteration 14:\tcost = 0.584\n",
      "iteration 15:\tcost = 0.575\n",
      "iteration 16:\tcost = 0.568\n",
      "iteration 17:\tcost = 0.561\n",
      "iteration 18:\tcost = 0.554\n",
      "iteration 19:\tcost = 0.548\n",
      "iteration 20:\tcost = 0.542\n",
      "{'AUC': 0.60849056603773577,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.78301886792452835,\n",
      " 'error_test': 0.16600000000000001,\n",
      " 'error_train': 0.184}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 8,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.881\n",
      "iteration 3:\tcost = 0.796\n",
      "iteration 4:\tcost = 0.734\n",
      "iteration 5:\tcost = 0.687\n",
      "iteration 6:\tcost = 0.651\n",
      "iteration 7:\tcost = 0.623\n",
      "iteration 8:\tcost = 0.601\n",
      "iteration 9:\tcost = 0.582\n",
      "iteration 10:\tcost = 0.566\n",
      "iteration 11:\tcost = 0.552\n",
      "iteration 12:\tcost = 0.540\n",
      "iteration 13:\tcost = 0.529\n",
      "iteration 14:\tcost = 0.519\n",
      "iteration 15:\tcost = 0.510\n",
      "iteration 16:\tcost = 0.502\n",
      "iteration 17:\tcost = 0.494\n",
      "iteration 18:\tcost = 0.486\n",
      "iteration 19:\tcost = 0.480\n",
      "iteration 20:\tcost = 0.473\n",
      "{'AUC': 0.57983193277310918,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.84033613445378152,\n",
      " 'error_test': 0.20000000000000001,\n",
      " 'error_train': 0.16800000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 9,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.892\n",
      "iteration 3:\tcost = 0.814\n",
      "iteration 4:\tcost = 0.757\n",
      "iteration 5:\tcost = 0.715\n",
      "iteration 6:\tcost = 0.682\n",
      "iteration 7:\tcost = 0.657\n",
      "iteration 8:\tcost = 0.637\n",
      "iteration 9:\tcost = 0.620\n",
      "iteration 10:\tcost = 0.605\n",
      "iteration 11:\tcost = 0.593\n",
      "iteration 12:\tcost = 0.582\n",
      "iteration 13:\tcost = 0.572\n",
      "iteration 14:\tcost = 0.563\n",
      "iteration 15:\tcost = 0.555\n",
      "iteration 16:\tcost = 0.548\n",
      "iteration 17:\tcost = 0.541\n",
      "iteration 18:\tcost = 0.534\n",
      "iteration 19:\tcost = 0.528\n",
      "iteration 20:\tcost = 0.523\n",
      "{'AUC': 0.57657657657657657,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.84684684684684686,\n",
      " 'error_test': 0.188,\n",
      " 'error_train': 0.17599999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 10,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.881\n",
      "iteration 3:\tcost = 0.797\n",
      "iteration 4:\tcost = 0.736\n",
      "iteration 5:\tcost = 0.691\n",
      "iteration 6:\tcost = 0.657\n",
      "iteration 7:\tcost = 0.631\n",
      "iteration 8:\tcost = 0.611\n",
      "iteration 9:\tcost = 0.594\n",
      "iteration 10:\tcost = 0.580\n",
      "iteration 11:\tcost = 0.569\n",
      "iteration 12:\tcost = 0.558\n",
      "iteration 13:\tcost = 0.549\n",
      "iteration 14:\tcost = 0.541\n",
      "iteration 15:\tcost = 0.534\n",
      "iteration 16:\tcost = 0.527\n",
      "iteration 17:\tcost = 0.521\n",
      "iteration 18:\tcost = 0.515\n",
      "iteration 19:\tcost = 0.509\n",
      "iteration 20:\tcost = 0.504\n",
      "{'AUC': 0.56302521008403361,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.87394957983193278,\n",
      " 'error_test': 0.20799999999999999,\n",
      " 'error_train': 0.17599999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 1,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.884\n",
      "iteration 3:\tcost = 0.801\n",
      "iteration 4:\tcost = 0.741\n",
      "iteration 5:\tcost = 0.695\n",
      "iteration 6:\tcost = 0.661\n",
      "iteration 7:\tcost = 0.635\n",
      "iteration 8:\tcost = 0.614\n",
      "iteration 9:\tcost = 0.596\n",
      "iteration 10:\tcost = 0.582\n",
      "iteration 11:\tcost = 0.569\n",
      "iteration 12:\tcost = 0.558\n",
      "iteration 13:\tcost = 0.548\n",
      "iteration 14:\tcost = 0.540\n",
      "iteration 15:\tcost = 0.531\n",
      "iteration 16:\tcost = 0.524\n",
      "iteration 17:\tcost = 0.517\n",
      "iteration 18:\tcost = 0.510\n",
      "iteration 19:\tcost = 0.504\n",
      "iteration 20:\tcost = 0.498\n",
      "{'AUC': 0.57407407407407407,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.85185185185185186,\n",
      " 'error_test': 0.184,\n",
      " 'error_train': 0.16200000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 2,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.870\n",
      "iteration 3:\tcost = 0.776\n",
      "iteration 4:\tcost = 0.709\n",
      "iteration 5:\tcost = 0.660\n",
      "iteration 6:\tcost = 0.623\n",
      "iteration 7:\tcost = 0.595\n",
      "iteration 8:\tcost = 0.573\n",
      "iteration 9:\tcost = 0.556\n",
      "iteration 10:\tcost = 0.541\n",
      "iteration 11:\tcost = 0.529\n",
      "iteration 12:\tcost = 0.519\n",
      "iteration 13:\tcost = 0.510\n",
      "iteration 14:\tcost = 0.501\n",
      "iteration 15:\tcost = 0.494\n",
      "iteration 16:\tcost = 0.487\n",
      "iteration 17:\tcost = 0.481\n",
      "iteration 18:\tcost = 0.475\n",
      "iteration 19:\tcost = 0.470\n",
      "iteration 20:\tcost = 0.464\n",
      "{'AUC': 0.51769911504424782,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.96460176991150437,\n",
      " 'error_test': 0.218,\n",
      " 'error_train': 0.16}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 3,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.883\n",
      "iteration 3:\tcost = 0.798\n",
      "iteration 4:\tcost = 0.737\n",
      "iteration 5:\tcost = 0.692\n",
      "iteration 6:\tcost = 0.658\n",
      "iteration 7:\tcost = 0.631\n",
      "iteration 8:\tcost = 0.611\n",
      "iteration 9:\tcost = 0.593\n",
      "iteration 10:\tcost = 0.579\n",
      "iteration 11:\tcost = 0.567\n",
      "iteration 12:\tcost = 0.556\n",
      "iteration 13:\tcost = 0.547\n",
      "iteration 14:\tcost = 0.538\n",
      "iteration 15:\tcost = 0.531\n",
      "iteration 16:\tcost = 0.523\n",
      "iteration 17:\tcost = 0.517\n",
      "iteration 18:\tcost = 0.511\n",
      "iteration 19:\tcost = 0.505\n",
      "iteration 20:\tcost = 0.499\n",
      "{'AUC': 0.55405405405405406,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.89189189189189189,\n",
      " 'error_test': 0.19800000000000001,\n",
      " 'error_train': 0.17000000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 4,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.879\n",
      "iteration 3:\tcost = 0.792\n",
      "iteration 4:\tcost = 0.727\n",
      "iteration 5:\tcost = 0.679\n",
      "iteration 6:\tcost = 0.641\n",
      "iteration 7:\tcost = 0.612\n",
      "iteration 8:\tcost = 0.588\n",
      "iteration 9:\tcost = 0.568\n",
      "iteration 10:\tcost = 0.551\n",
      "iteration 11:\tcost = 0.536\n",
      "iteration 12:\tcost = 0.523\n",
      "iteration 13:\tcost = 0.512\n",
      "iteration 14:\tcost = 0.501\n",
      "iteration 15:\tcost = 0.492\n",
      "iteration 16:\tcost = 0.483\n",
      "iteration 17:\tcost = 0.475\n",
      "iteration 18:\tcost = 0.468\n",
      "iteration 19:\tcost = 0.461\n",
      "iteration 20:\tcost = 0.454\n",
      "{'AUC': 0.56730769230769229,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.86538461538461542,\n",
      " 'error_test': 0.17999999999999999,\n",
      " 'error_train': 0.14000000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 5,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.890\n",
      "iteration 3:\tcost = 0.810\n",
      "iteration 4:\tcost = 0.751\n",
      "iteration 5:\tcost = 0.708\n",
      "iteration 6:\tcost = 0.675\n",
      "iteration 7:\tcost = 0.649\n",
      "iteration 8:\tcost = 0.629\n",
      "iteration 9:\tcost = 0.612\n",
      "iteration 10:\tcost = 0.598\n",
      "iteration 11:\tcost = 0.585\n",
      "iteration 12:\tcost = 0.575\n",
      "iteration 13:\tcost = 0.565\n",
      "iteration 14:\tcost = 0.557\n",
      "iteration 15:\tcost = 0.549\n",
      "iteration 16:\tcost = 0.542\n",
      "iteration 17:\tcost = 0.535\n",
      "iteration 18:\tcost = 0.529\n",
      "iteration 19:\tcost = 0.523\n",
      "iteration 20:\tcost = 0.517\n",
      "{'AUC': 0.56435643564356441,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.87128712871287128,\n",
      " 'error_test': 0.17599999999999999,\n",
      " 'error_train': 0.16800000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 6,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.892\n",
      "iteration 3:\tcost = 0.814\n",
      "iteration 4:\tcost = 0.756\n",
      "iteration 5:\tcost = 0.713\n",
      "iteration 6:\tcost = 0.680\n",
      "iteration 7:\tcost = 0.654\n",
      "iteration 8:\tcost = 0.633\n",
      "iteration 9:\tcost = 0.615\n",
      "iteration 10:\tcost = 0.600\n",
      "iteration 11:\tcost = 0.588\n",
      "iteration 12:\tcost = 0.576\n",
      "iteration 13:\tcost = 0.566\n",
      "iteration 14:\tcost = 0.557\n",
      "iteration 15:\tcost = 0.549\n",
      "iteration 16:\tcost = 0.541\n",
      "iteration 17:\tcost = 0.534\n",
      "iteration 18:\tcost = 0.528\n",
      "iteration 19:\tcost = 0.522\n",
      "iteration 20:\tcost = 0.516\n",
      "{'AUC': 0.58333333333333326,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.83333333333333337,\n",
      " 'error_test': 0.17000000000000001,\n",
      " 'error_train': 0.16600000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 7,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.895\n",
      "iteration 3:\tcost = 0.819\n",
      "iteration 4:\tcost = 0.763\n",
      "iteration 5:\tcost = 0.721\n",
      "iteration 6:\tcost = 0.689\n",
      "iteration 7:\tcost = 0.663\n",
      "iteration 8:\tcost = 0.643\n",
      "iteration 9:\tcost = 0.625\n",
      "iteration 10:\tcost = 0.611\n",
      "iteration 11:\tcost = 0.598\n",
      "iteration 12:\tcost = 0.587\n",
      "iteration 13:\tcost = 0.577\n",
      "iteration 14:\tcost = 0.568\n",
      "iteration 15:\tcost = 0.560\n",
      "iteration 16:\tcost = 0.552\n",
      "iteration 17:\tcost = 0.545\n",
      "iteration 18:\tcost = 0.538\n",
      "iteration 19:\tcost = 0.532\n",
      "iteration 20:\tcost = 0.527\n",
      "{'AUC': 0.57070707070707072,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.85858585858585856,\n",
      " 'error_test': 0.17000000000000001,\n",
      " 'error_train': 0.16800000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 8,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.879\n",
      "iteration 3:\tcost = 0.793\n",
      "iteration 4:\tcost = 0.730\n",
      "iteration 5:\tcost = 0.684\n",
      "iteration 6:\tcost = 0.649\n",
      "iteration 7:\tcost = 0.622\n",
      "iteration 8:\tcost = 0.600\n",
      "iteration 9:\tcost = 0.583\n",
      "iteration 10:\tcost = 0.569\n",
      "iteration 11:\tcost = 0.556\n",
      "iteration 12:\tcost = 0.546\n",
      "iteration 13:\tcost = 0.536\n",
      "iteration 14:\tcost = 0.528\n",
      "iteration 15:\tcost = 0.520\n",
      "iteration 16:\tcost = 0.513\n",
      "iteration 17:\tcost = 0.506\n",
      "iteration 18:\tcost = 0.500\n",
      "iteration 19:\tcost = 0.494\n",
      "iteration 20:\tcost = 0.488\n",
      "{'AUC': 0.56578947368421051,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.86842105263157898,\n",
      " 'error_test': 0.19800000000000001,\n",
      " 'error_train': 0.16800000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 9,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.889\n",
      "iteration 3:\tcost = 0.809\n",
      "iteration 4:\tcost = 0.751\n",
      "iteration 5:\tcost = 0.707\n",
      "iteration 6:\tcost = 0.674\n",
      "iteration 7:\tcost = 0.649\n",
      "iteration 8:\tcost = 0.629\n",
      "iteration 9:\tcost = 0.612\n",
      "iteration 10:\tcost = 0.598\n",
      "iteration 11:\tcost = 0.586\n",
      "iteration 12:\tcost = 0.576\n",
      "iteration 13:\tcost = 0.567\n",
      "iteration 14:\tcost = 0.558\n",
      "iteration 15:\tcost = 0.551\n",
      "iteration 16:\tcost = 0.544\n",
      "iteration 17:\tcost = 0.537\n",
      "iteration 18:\tcost = 0.531\n",
      "iteration 19:\tcost = 0.526\n",
      "iteration 20:\tcost = 0.520\n",
      "{'AUC': 0.56074766355140193,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.87850467289719625,\n",
      " 'error_test': 0.188,\n",
      " 'error_train': 0.17999999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 10,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.867\n",
      "iteration 3:\tcost = 0.773\n",
      "iteration 4:\tcost = 0.704\n",
      "iteration 5:\tcost = 0.653\n",
      "iteration 6:\tcost = 0.615\n",
      "iteration 7:\tcost = 0.586\n",
      "iteration 8:\tcost = 0.563\n",
      "iteration 9:\tcost = 0.544\n",
      "iteration 10:\tcost = 0.529\n",
      "iteration 11:\tcost = 0.516\n",
      "iteration 12:\tcost = 0.504\n",
      "iteration 13:\tcost = 0.494\n",
      "iteration 14:\tcost = 0.485\n",
      "iteration 15:\tcost = 0.477\n",
      "iteration 16:\tcost = 0.470\n",
      "iteration 17:\tcost = 0.463\n",
      "iteration 18:\tcost = 0.456\n",
      "iteration 19:\tcost = 0.451\n",
      "iteration 20:\tcost = 0.445\n",
      "{'AUC': 0.53813559322033899,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.92372881355932202,\n",
      " 'error_test': 0.218,\n",
      " 'error_train': 0.14999999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 1,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.877\n",
      "iteration 3:\tcost = 0.787\n",
      "iteration 4:\tcost = 0.721\n",
      "iteration 5:\tcost = 0.672\n",
      "iteration 6:\tcost = 0.634\n",
      "iteration 7:\tcost = 0.604\n",
      "iteration 8:\tcost = 0.580\n",
      "iteration 9:\tcost = 0.560\n",
      "iteration 10:\tcost = 0.543\n",
      "iteration 11:\tcost = 0.528\n",
      "iteration 12:\tcost = 0.516\n",
      "iteration 13:\tcost = 0.504\n",
      "iteration 14:\tcost = 0.494\n",
      "iteration 15:\tcost = 0.484\n",
      "iteration 16:\tcost = 0.476\n",
      "iteration 17:\tcost = 0.468\n",
      "iteration 18:\tcost = 0.460\n",
      "iteration 19:\tcost = 0.453\n",
      "iteration 20:\tcost = 0.447\n",
      "{'AUC': 0.55825242718446599,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.88349514563106801,\n",
      " 'error_test': 0.182,\n",
      " 'error_train': 0.14799999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 2,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.854\n",
      "iteration 3:\tcost = 0.750\n",
      "iteration 4:\tcost = 0.674\n",
      "iteration 5:\tcost = 0.619\n",
      "iteration 6:\tcost = 0.578\n",
      "iteration 7:\tcost = 0.546\n",
      "iteration 8:\tcost = 0.522\n",
      "iteration 9:\tcost = 0.502\n",
      "iteration 10:\tcost = 0.486\n",
      "iteration 11:\tcost = 0.472\n",
      "iteration 12:\tcost = 0.461\n",
      "iteration 13:\tcost = 0.450\n",
      "iteration 14:\tcost = 0.441\n",
      "iteration 15:\tcost = 0.433\n",
      "iteration 16:\tcost = 0.426\n",
      "iteration 17:\tcost = 0.419\n",
      "iteration 18:\tcost = 0.412\n",
      "iteration 19:\tcost = 0.406\n",
      "iteration 20:\tcost = 0.401\n",
      "{'AUC': 0.52631578947368429,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.94736842105263153,\n",
      " 'error_test': 0.216,\n",
      " 'error_train': 0.14000000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 3,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.881\n",
      "iteration 3:\tcost = 0.796\n",
      "iteration 4:\tcost = 0.734\n",
      "iteration 5:\tcost = 0.688\n",
      "iteration 6:\tcost = 0.654\n",
      "iteration 7:\tcost = 0.627\n",
      "iteration 8:\tcost = 0.606\n",
      "iteration 9:\tcost = 0.589\n",
      "iteration 10:\tcost = 0.574\n",
      "iteration 11:\tcost = 0.562\n",
      "iteration 12:\tcost = 0.552\n",
      "iteration 13:\tcost = 0.542\n",
      "iteration 14:\tcost = 0.534\n",
      "iteration 15:\tcost = 0.526\n",
      "iteration 16:\tcost = 0.519\n",
      "iteration 17:\tcost = 0.513\n",
      "iteration 18:\tcost = 0.507\n",
      "iteration 19:\tcost = 0.501\n",
      "iteration 20:\tcost = 0.496\n",
      "{'AUC': 0.55504587155963303,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.88990825688073394,\n",
      " 'error_test': 0.19400000000000001,\n",
      " 'error_train': 0.17000000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 4,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.869\n",
      "iteration 3:\tcost = 0.774\n",
      "iteration 4:\tcost = 0.705\n",
      "iteration 5:\tcost = 0.653\n",
      "iteration 6:\tcost = 0.614\n",
      "iteration 7:\tcost = 0.583\n",
      "iteration 8:\tcost = 0.558\n",
      "iteration 9:\tcost = 0.538\n",
      "iteration 10:\tcost = 0.521\n",
      "iteration 11:\tcost = 0.506\n",
      "iteration 12:\tcost = 0.493\n",
      "iteration 13:\tcost = 0.482\n",
      "iteration 14:\tcost = 0.472\n",
      "iteration 15:\tcost = 0.463\n",
      "iteration 16:\tcost = 0.454\n",
      "iteration 17:\tcost = 0.446\n",
      "iteration 18:\tcost = 0.439\n",
      "iteration 19:\tcost = 0.432\n",
      "iteration 20:\tcost = 0.426\n",
      "{'AUC': 0.54954954954954949,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.90090090090090091,\n",
      " 'error_test': 0.20000000000000001,\n",
      " 'error_train': 0.14999999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 5,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.865\n",
      "iteration 3:\tcost = 0.768\n",
      "iteration 4:\tcost = 0.698\n",
      "iteration 5:\tcost = 0.647\n",
      "iteration 6:\tcost = 0.609\n",
      "iteration 7:\tcost = 0.580\n",
      "iteration 8:\tcost = 0.557\n",
      "iteration 9:\tcost = 0.539\n",
      "iteration 10:\tcost = 0.524\n",
      "iteration 11:\tcost = 0.512\n",
      "iteration 12:\tcost = 0.501\n",
      "iteration 13:\tcost = 0.491\n",
      "iteration 14:\tcost = 0.483\n",
      "iteration 15:\tcost = 0.475\n",
      "iteration 16:\tcost = 0.468\n",
      "iteration 17:\tcost = 0.461\n",
      "iteration 18:\tcost = 0.455\n",
      "iteration 19:\tcost = 0.450\n",
      "iteration 20:\tcost = 0.444\n",
      "{'AUC': 0.54545454545454541,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.90909090909090906,\n",
      " 'error_test': 0.20000000000000001,\n",
      " 'error_train': 0.158}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 6,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.859\n",
      "iteration 3:\tcost = 0.757\n",
      "iteration 4:\tcost = 0.683\n",
      "iteration 5:\tcost = 0.629\n",
      "iteration 6:\tcost = 0.588\n",
      "iteration 7:\tcost = 0.557\n",
      "iteration 8:\tcost = 0.532\n",
      "iteration 9:\tcost = 0.512\n",
      "iteration 10:\tcost = 0.495\n",
      "iteration 11:\tcost = 0.481\n",
      "iteration 12:\tcost = 0.469\n",
      "iteration 13:\tcost = 0.458\n",
      "iteration 14:\tcost = 0.449\n",
      "iteration 15:\tcost = 0.440\n",
      "iteration 16:\tcost = 0.432\n",
      "iteration 17:\tcost = 0.425\n",
      "iteration 18:\tcost = 0.418\n",
      "iteration 19:\tcost = 0.412\n",
      "iteration 20:\tcost = 0.406\n",
      "{'AUC': 0.52212389380530966,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.95575221238938057,\n",
      " 'error_test': 0.216,\n",
      " 'error_train': 0.14199999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 7,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.871\n",
      "iteration 3:\tcost = 0.780\n",
      "iteration 4:\tcost = 0.714\n",
      "iteration 5:\tcost = 0.667\n",
      "iteration 6:\tcost = 0.632\n",
      "iteration 7:\tcost = 0.605\n",
      "iteration 8:\tcost = 0.585\n",
      "iteration 9:\tcost = 0.569\n",
      "iteration 10:\tcost = 0.557\n",
      "iteration 11:\tcost = 0.546\n",
      "iteration 12:\tcost = 0.538\n",
      "iteration 13:\tcost = 0.530\n",
      "iteration 14:\tcost = 0.523\n",
      "iteration 15:\tcost = 0.517\n",
      "iteration 16:\tcost = 0.512\n",
      "iteration 17:\tcost = 0.507\n",
      "iteration 18:\tcost = 0.502\n",
      "iteration 19:\tcost = 0.497\n",
      "iteration 20:\tcost = 0.493\n",
      "{'AUC': 0.52358490566037741,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.95283018867924529,\n",
      " 'error_test': 0.20200000000000001,\n",
      " 'error_train': 0.17000000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 8,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.864\n",
      "iteration 3:\tcost = 0.767\n",
      "iteration 4:\tcost = 0.696\n",
      "iteration 5:\tcost = 0.643\n",
      "iteration 6:\tcost = 0.603\n",
      "iteration 7:\tcost = 0.573\n",
      "iteration 8:\tcost = 0.548\n",
      "iteration 9:\tcost = 0.529\n",
      "iteration 10:\tcost = 0.512\n",
      "iteration 11:\tcost = 0.498\n",
      "iteration 12:\tcost = 0.486\n",
      "iteration 13:\tcost = 0.476\n",
      "iteration 14:\tcost = 0.466\n",
      "iteration 15:\tcost = 0.458\n",
      "iteration 16:\tcost = 0.450\n",
      "iteration 17:\tcost = 0.443\n",
      "iteration 18:\tcost = 0.436\n",
      "iteration 19:\tcost = 0.430\n",
      "iteration 20:\tcost = 0.424\n",
      "{'AUC': 0.53211009174311918,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.93577981651376152,\n",
      " 'error_test': 0.20399999999999999,\n",
      " 'error_train': 0.14199999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 9,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.866\n",
      "iteration 3:\tcost = 0.771\n",
      "iteration 4:\tcost = 0.701\n",
      "iteration 5:\tcost = 0.650\n",
      "iteration 6:\tcost = 0.612\n",
      "iteration 7:\tcost = 0.583\n",
      "iteration 8:\tcost = 0.560\n",
      "iteration 9:\tcost = 0.542\n",
      "iteration 10:\tcost = 0.527\n",
      "iteration 11:\tcost = 0.514\n",
      "iteration 12:\tcost = 0.504\n",
      "iteration 13:\tcost = 0.494\n",
      "iteration 14:\tcost = 0.485\n",
      "iteration 15:\tcost = 0.478\n",
      "iteration 16:\tcost = 0.471\n",
      "iteration 17:\tcost = 0.464\n",
      "iteration 18:\tcost = 0.458\n",
      "iteration 19:\tcost = 0.453\n",
      "iteration 20:\tcost = 0.448\n",
      "{'AUC': 0.5267857142857143,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.9464285714285714,\n",
      " 'error_test': 0.21199999999999999,\n",
      " 'error_train': 0.16200000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 10,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.865\n",
      "iteration 3:\tcost = 0.768\n",
      "iteration 4:\tcost = 0.699\n",
      "iteration 5:\tcost = 0.647\n",
      "iteration 6:\tcost = 0.609\n",
      "iteration 7:\tcost = 0.579\n",
      "iteration 8:\tcost = 0.556\n",
      "iteration 9:\tcost = 0.538\n",
      "iteration 10:\tcost = 0.522\n",
      "iteration 11:\tcost = 0.510\n",
      "iteration 12:\tcost = 0.499\n",
      "iteration 13:\tcost = 0.489\n",
      "iteration 14:\tcost = 0.480\n",
      "iteration 15:\tcost = 0.472\n",
      "iteration 16:\tcost = 0.465\n",
      "iteration 17:\tcost = 0.459\n",
      "iteration 18:\tcost = 0.453\n",
      "iteration 19:\tcost = 0.447\n",
      "iteration 20:\tcost = 0.442\n",
      "{'AUC': 0.55309734513274345,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.89380530973451322,\n",
      " 'error_test': 0.20200000000000001,\n",
      " 'error_train': 0.154}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 1,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.855\n",
      "iteration 3:\tcost = 0.752\n",
      "iteration 4:\tcost = 0.678\n",
      "iteration 5:\tcost = 0.625\n",
      "iteration 6:\tcost = 0.586\n",
      "iteration 7:\tcost = 0.557\n",
      "iteration 8:\tcost = 0.535\n",
      "iteration 9:\tcost = 0.518\n",
      "iteration 10:\tcost = 0.505\n",
      "iteration 11:\tcost = 0.494\n",
      "iteration 12:\tcost = 0.484\n",
      "iteration 13:\tcost = 0.476\n",
      "iteration 14:\tcost = 0.470\n",
      "iteration 15:\tcost = 0.463\n",
      "iteration 16:\tcost = 0.458\n",
      "iteration 17:\tcost = 0.453\n",
      "iteration 18:\tcost = 0.448\n",
      "iteration 19:\tcost = 0.444\n",
      "iteration 20:\tcost = 0.440\n",
      "{'AUC': 0.50934579439252337,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.98130841121495327,\n",
      " 'error_test': 0.20999999999999999,\n",
      " 'error_train': 0.14799999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 2,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.861\n",
      "iteration 3:\tcost = 0.761\n",
      "iteration 4:\tcost = 0.688\n",
      "iteration 5:\tcost = 0.633\n",
      "iteration 6:\tcost = 0.592\n",
      "iteration 7:\tcost = 0.560\n",
      "iteration 8:\tcost = 0.535\n",
      "iteration 9:\tcost = 0.515\n",
      "iteration 10:\tcost = 0.498\n",
      "iteration 11:\tcost = 0.483\n",
      "iteration 12:\tcost = 0.470\n",
      "iteration 13:\tcost = 0.459\n",
      "iteration 14:\tcost = 0.449\n",
      "iteration 15:\tcost = 0.440\n",
      "iteration 16:\tcost = 0.432\n",
      "iteration 17:\tcost = 0.424\n",
      "iteration 18:\tcost = 0.417\n",
      "iteration 19:\tcost = 0.411\n",
      "iteration 20:\tcost = 0.405\n",
      "{'AUC': 0.54285714285714293,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.91428571428571426,\n",
      " 'error_test': 0.192,\n",
      " 'error_train': 0.13800000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 3,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.837\n",
      "iteration 3:\tcost = 0.722\n",
      "iteration 4:\tcost = 0.640\n",
      "iteration 5:\tcost = 0.581\n",
      "iteration 6:\tcost = 0.538\n",
      "iteration 7:\tcost = 0.507\n",
      "iteration 8:\tcost = 0.483\n",
      "iteration 9:\tcost = 0.465\n",
      "iteration 10:\tcost = 0.451\n",
      "iteration 11:\tcost = 0.440\n",
      "iteration 12:\tcost = 0.430\n",
      "iteration 13:\tcost = 0.422\n",
      "iteration 14:\tcost = 0.415\n",
      "iteration 15:\tcost = 0.409\n",
      "iteration 16:\tcost = 0.404\n",
      "iteration 17:\tcost = 0.399\n",
      "iteration 18:\tcost = 0.394\n",
      "iteration 19:\tcost = 0.390\n",
      "iteration 20:\tcost = 0.386\n",
      "{'AUC': 0.50877192982456143,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.98245614035087714,\n",
      " 'error_test': 0.224,\n",
      " 'error_train': 0.13200000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 4,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.849\n",
      "iteration 3:\tcost = 0.742\n",
      "iteration 4:\tcost = 0.666\n",
      "iteration 5:\tcost = 0.610\n",
      "iteration 6:\tcost = 0.569\n",
      "iteration 7:\tcost = 0.538\n",
      "iteration 8:\tcost = 0.515\n",
      "iteration 9:\tcost = 0.496\n",
      "iteration 10:\tcost = 0.481\n",
      "iteration 11:\tcost = 0.469\n",
      "iteration 12:\tcost = 0.458\n",
      "iteration 13:\tcost = 0.449\n",
      "iteration 14:\tcost = 0.441\n",
      "iteration 15:\tcost = 0.434\n",
      "iteration 16:\tcost = 0.428\n",
      "iteration 17:\tcost = 0.421\n",
      "iteration 18:\tcost = 0.416\n",
      "iteration 19:\tcost = 0.410\n",
      "iteration 20:\tcost = 0.406\n",
      "{'AUC': 0.5267857142857143,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.9464285714285714,\n",
      " 'error_test': 0.21199999999999999,\n",
      " 'error_train': 0.14199999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 5,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.837\n",
      "iteration 3:\tcost = 0.722\n",
      "iteration 4:\tcost = 0.640\n",
      "iteration 5:\tcost = 0.581\n",
      "iteration 6:\tcost = 0.538\n",
      "iteration 7:\tcost = 0.507\n",
      "iteration 8:\tcost = 0.483\n",
      "iteration 9:\tcost = 0.465\n",
      "iteration 10:\tcost = 0.450\n",
      "iteration 11:\tcost = 0.439\n",
      "iteration 12:\tcost = 0.429\n",
      "iteration 13:\tcost = 0.421\n",
      "iteration 14:\tcost = 0.413\n",
      "iteration 15:\tcost = 0.407\n",
      "iteration 16:\tcost = 0.401\n",
      "iteration 17:\tcost = 0.396\n",
      "iteration 18:\tcost = 0.391\n",
      "iteration 19:\tcost = 0.386\n",
      "iteration 20:\tcost = 0.382\n",
      "{'AUC': 0.5083333333333333,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.98333333333333328,\n",
      " 'error_test': 0.23599999999999999,\n",
      " 'error_train': 0.13}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 6,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.846\n",
      "iteration 3:\tcost = 0.737\n",
      "iteration 4:\tcost = 0.659\n",
      "iteration 5:\tcost = 0.602\n",
      "iteration 6:\tcost = 0.561\n",
      "iteration 7:\tcost = 0.530\n",
      "iteration 8:\tcost = 0.506\n",
      "iteration 9:\tcost = 0.487\n",
      "iteration 10:\tcost = 0.472\n",
      "iteration 11:\tcost = 0.460\n",
      "iteration 12:\tcost = 0.450\n",
      "iteration 13:\tcost = 0.441\n",
      "iteration 14:\tcost = 0.433\n",
      "iteration 15:\tcost = 0.426\n",
      "iteration 16:\tcost = 0.419\n",
      "iteration 17:\tcost = 0.414\n",
      "iteration 18:\tcost = 0.408\n",
      "iteration 19:\tcost = 0.403\n",
      "iteration 20:\tcost = 0.398\n",
      "{'AUC': 0.50862068965517238,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.98275862068965514,\n",
      " 'error_test': 0.22800000000000001,\n",
      " 'error_train': 0.13800000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 7,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.879\n",
      "iteration 3:\tcost = 0.792\n",
      "iteration 4:\tcost = 0.728\n",
      "iteration 5:\tcost = 0.680\n",
      "iteration 6:\tcost = 0.643\n",
      "iteration 7:\tcost = 0.614\n",
      "iteration 8:\tcost = 0.592\n",
      "iteration 9:\tcost = 0.573\n",
      "iteration 10:\tcost = 0.557\n",
      "iteration 11:\tcost = 0.544\n",
      "iteration 12:\tcost = 0.532\n",
      "iteration 13:\tcost = 0.522\n",
      "iteration 14:\tcost = 0.513\n",
      "iteration 15:\tcost = 0.504\n",
      "iteration 16:\tcost = 0.497\n",
      "iteration 17:\tcost = 0.489\n",
      "iteration 18:\tcost = 0.483\n",
      "iteration 19:\tcost = 0.477\n",
      "iteration 20:\tcost = 0.471\n",
      "{'AUC': 0.54081632653061229,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.91836734693877553,\n",
      " 'error_test': 0.17999999999999999,\n",
      " 'error_train': 0.152}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 8,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.859\n",
      "iteration 3:\tcost = 0.759\n",
      "iteration 4:\tcost = 0.686\n",
      "iteration 5:\tcost = 0.633\n",
      "iteration 6:\tcost = 0.593\n",
      "iteration 7:\tcost = 0.563\n",
      "iteration 8:\tcost = 0.540\n",
      "iteration 9:\tcost = 0.521\n",
      "iteration 10:\tcost = 0.506\n",
      "iteration 11:\tcost = 0.494\n",
      "iteration 12:\tcost = 0.483\n",
      "iteration 13:\tcost = 0.474\n",
      "iteration 14:\tcost = 0.466\n",
      "iteration 15:\tcost = 0.458\n",
      "iteration 16:\tcost = 0.452\n",
      "iteration 17:\tcost = 0.446\n",
      "iteration 18:\tcost = 0.440\n",
      "iteration 19:\tcost = 0.435\n",
      "iteration 20:\tcost = 0.430\n",
      "{'AUC': 0.5185185185185186,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.96296296296296291,\n",
      " 'error_test': 0.20799999999999999,\n",
      " 'error_train': 0.14399999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 9,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.861\n",
      "iteration 3:\tcost = 0.762\n",
      "iteration 4:\tcost = 0.690\n",
      "iteration 5:\tcost = 0.637\n",
      "iteration 6:\tcost = 0.598\n",
      "iteration 7:\tcost = 0.568\n",
      "iteration 8:\tcost = 0.544\n",
      "iteration 9:\tcost = 0.525\n",
      "iteration 10:\tcost = 0.510\n",
      "iteration 11:\tcost = 0.497\n",
      "iteration 12:\tcost = 0.486\n",
      "iteration 13:\tcost = 0.476\n",
      "iteration 14:\tcost = 0.468\n",
      "iteration 15:\tcost = 0.460\n",
      "iteration 16:\tcost = 0.452\n",
      "iteration 17:\tcost = 0.446\n",
      "iteration 18:\tcost = 0.439\n",
      "iteration 19:\tcost = 0.433\n",
      "iteration 20:\tcost = 0.428\n",
      "{'AUC': 0.52659574468085113,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.94680851063829785,\n",
      " 'error_test': 0.17799999999999999,\n",
      " 'error_train': 0.14799999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 10,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.858\n",
      "iteration 3:\tcost = 0.756\n",
      "iteration 4:\tcost = 0.682\n",
      "iteration 5:\tcost = 0.627\n",
      "iteration 6:\tcost = 0.585\n",
      "iteration 7:\tcost = 0.553\n",
      "iteration 8:\tcost = 0.528\n",
      "iteration 9:\tcost = 0.508\n",
      "iteration 10:\tcost = 0.491\n",
      "iteration 11:\tcost = 0.477\n",
      "iteration 12:\tcost = 0.465\n",
      "iteration 13:\tcost = 0.454\n",
      "iteration 14:\tcost = 0.445\n",
      "iteration 15:\tcost = 0.436\n",
      "iteration 16:\tcost = 0.429\n",
      "iteration 17:\tcost = 0.421\n",
      "iteration 18:\tcost = 0.415\n",
      "iteration 19:\tcost = 0.409\n",
      "iteration 20:\tcost = 0.403\n",
      "{'AUC': 0.54464285714285721,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.9107142857142857,\n",
      " 'error_test': 0.20399999999999999,\n",
      " 'error_train': 0.14199999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 1,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.839\n",
      "iteration 3:\tcost = 0.725\n",
      "iteration 4:\tcost = 0.644\n",
      "iteration 5:\tcost = 0.586\n",
      "iteration 6:\tcost = 0.544\n",
      "iteration 7:\tcost = 0.513\n",
      "iteration 8:\tcost = 0.489\n",
      "iteration 9:\tcost = 0.471\n",
      "iteration 10:\tcost = 0.457\n",
      "iteration 11:\tcost = 0.446\n",
      "iteration 12:\tcost = 0.437\n",
      "iteration 13:\tcost = 0.429\n",
      "iteration 14:\tcost = 0.422\n",
      "iteration 15:\tcost = 0.416\n",
      "iteration 16:\tcost = 0.410\n",
      "iteration 17:\tcost = 0.405\n",
      "iteration 18:\tcost = 0.401\n",
      "iteration 19:\tcost = 0.396\n",
      "iteration 20:\tcost = 0.392\n",
      "{'AUC': 0.51415094339622636,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.97169811320754718,\n",
      " 'error_test': 0.20599999999999999,\n",
      " 'error_train': 0.13400000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 2,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.837\n",
      "iteration 3:\tcost = 0.721\n",
      "iteration 4:\tcost = 0.638\n",
      "iteration 5:\tcost = 0.579\n",
      "iteration 6:\tcost = 0.535\n",
      "iteration 7:\tcost = 0.503\n",
      "iteration 8:\tcost = 0.479\n",
      "iteration 9:\tcost = 0.460\n",
      "iteration 10:\tcost = 0.445\n",
      "iteration 11:\tcost = 0.434\n",
      "iteration 12:\tcost = 0.424\n",
      "iteration 13:\tcost = 0.415\n",
      "iteration 14:\tcost = 0.408\n",
      "iteration 15:\tcost = 0.402\n",
      "iteration 16:\tcost = 0.396\n",
      "iteration 17:\tcost = 0.390\n",
      "iteration 18:\tcost = 0.385\n",
      "iteration 19:\tcost = 0.381\n",
      "iteration 20:\tcost = 0.376\n",
      "{'AUC': 0.51694915254237284,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.96610169491525422,\n",
      " 'error_test': 0.22800000000000001,\n",
      " 'error_train': 0.13400000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 3,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.846\n",
      "iteration 3:\tcost = 0.737\n",
      "iteration 4:\tcost = 0.659\n",
      "iteration 5:\tcost = 0.603\n",
      "iteration 6:\tcost = 0.562\n",
      "iteration 7:\tcost = 0.531\n",
      "iteration 8:\tcost = 0.508\n",
      "iteration 9:\tcost = 0.490\n",
      "iteration 10:\tcost = 0.475\n",
      "iteration 11:\tcost = 0.463\n",
      "iteration 12:\tcost = 0.453\n",
      "iteration 13:\tcost = 0.445\n",
      "iteration 14:\tcost = 0.438\n",
      "iteration 15:\tcost = 0.431\n",
      "iteration 16:\tcost = 0.425\n",
      "iteration 17:\tcost = 0.419\n",
      "iteration 18:\tcost = 0.414\n",
      "iteration 19:\tcost = 0.409\n",
      "iteration 20:\tcost = 0.404\n",
      "{'AUC': 0.51470588235294112,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.97058823529411764,\n",
      " 'error_test': 0.19800000000000001,\n",
      " 'error_train': 0.14199999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 4,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.863\n",
      "iteration 3:\tcost = 0.766\n",
      "iteration 4:\tcost = 0.695\n",
      "iteration 5:\tcost = 0.643\n",
      "iteration 6:\tcost = 0.605\n",
      "iteration 7:\tcost = 0.576\n",
      "iteration 8:\tcost = 0.553\n",
      "iteration 9:\tcost = 0.535\n",
      "iteration 10:\tcost = 0.521\n",
      "iteration 11:\tcost = 0.509\n",
      "iteration 12:\tcost = 0.498\n",
      "iteration 13:\tcost = 0.489\n",
      "iteration 14:\tcost = 0.481\n",
      "iteration 15:\tcost = 0.474\n",
      "iteration 16:\tcost = 0.467\n",
      "iteration 17:\tcost = 0.460\n",
      "iteration 18:\tcost = 0.455\n",
      "iteration 19:\tcost = 0.449\n",
      "iteration 20:\tcost = 0.444\n",
      "{'AUC': 0.52840909090909083,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.94318181818181823,\n",
      " 'error_test': 0.16600000000000001,\n",
      " 'error_train': 0.156}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 5,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.841\n",
      "iteration 3:\tcost = 0.728\n",
      "iteration 4:\tcost = 0.647\n",
      "iteration 5:\tcost = 0.589\n",
      "iteration 6:\tcost = 0.546\n",
      "iteration 7:\tcost = 0.514\n",
      "iteration 8:\tcost = 0.490\n",
      "iteration 9:\tcost = 0.471\n",
      "iteration 10:\tcost = 0.456\n",
      "iteration 11:\tcost = 0.444\n",
      "iteration 12:\tcost = 0.433\n",
      "iteration 13:\tcost = 0.425\n",
      "iteration 14:\tcost = 0.417\n",
      "iteration 15:\tcost = 0.410\n",
      "iteration 16:\tcost = 0.404\n",
      "iteration 17:\tcost = 0.398\n",
      "iteration 18:\tcost = 0.392\n",
      "iteration 19:\tcost = 0.387\n",
      "iteration 20:\tcost = 0.383\n",
      "{'AUC': 0.52136752136752129,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.95726495726495731,\n",
      " 'error_test': 0.224,\n",
      " 'error_train': 0.13400000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 6,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.854\n",
      "iteration 3:\tcost = 0.750\n",
      "iteration 4:\tcost = 0.675\n",
      "iteration 5:\tcost = 0.621\n",
      "iteration 6:\tcost = 0.581\n",
      "iteration 7:\tcost = 0.551\n",
      "iteration 8:\tcost = 0.528\n",
      "iteration 9:\tcost = 0.510\n",
      "iteration 10:\tcost = 0.495\n",
      "iteration 11:\tcost = 0.484\n",
      "iteration 12:\tcost = 0.474\n",
      "iteration 13:\tcost = 0.465\n",
      "iteration 14:\tcost = 0.458\n",
      "iteration 15:\tcost = 0.451\n",
      "iteration 16:\tcost = 0.445\n",
      "iteration 17:\tcost = 0.439\n",
      "iteration 18:\tcost = 0.434\n",
      "iteration 19:\tcost = 0.429\n",
      "iteration 20:\tcost = 0.424\n",
      "{'AUC': 0.51282051282051277,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.97435897435897434,\n",
      " 'error_test': 0.22800000000000001,\n",
      " 'error_train': 0.14999999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 7,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.865\n",
      "iteration 3:\tcost = 0.768\n",
      "iteration 4:\tcost = 0.696\n",
      "iteration 5:\tcost = 0.643\n",
      "iteration 6:\tcost = 0.603\n",
      "iteration 7:\tcost = 0.571\n",
      "iteration 8:\tcost = 0.546\n",
      "iteration 9:\tcost = 0.526\n",
      "iteration 10:\tcost = 0.509\n",
      "iteration 11:\tcost = 0.495\n",
      "iteration 12:\tcost = 0.482\n",
      "iteration 13:\tcost = 0.471\n",
      "iteration 14:\tcost = 0.461\n",
      "iteration 15:\tcost = 0.452\n",
      "iteration 16:\tcost = 0.444\n",
      "iteration 17:\tcost = 0.437\n",
      "iteration 18:\tcost = 0.430\n",
      "iteration 19:\tcost = 0.423\n",
      "iteration 20:\tcost = 0.417\n",
      "{'AUC': 0.53000000000000003,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.93999999999999995,\n",
      " 'error_test': 0.188,\n",
      " 'error_train': 0.14199999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 8,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.859\n",
      "iteration 3:\tcost = 0.759\n",
      "iteration 4:\tcost = 0.687\n",
      "iteration 5:\tcost = 0.634\n",
      "iteration 6:\tcost = 0.595\n",
      "iteration 7:\tcost = 0.565\n",
      "iteration 8:\tcost = 0.543\n",
      "iteration 9:\tcost = 0.524\n",
      "iteration 10:\tcost = 0.510\n",
      "iteration 11:\tcost = 0.497\n",
      "iteration 12:\tcost = 0.487\n",
      "iteration 13:\tcost = 0.478\n",
      "iteration 14:\tcost = 0.470\n",
      "iteration 15:\tcost = 0.462\n",
      "iteration 16:\tcost = 0.455\n",
      "iteration 17:\tcost = 0.449\n",
      "iteration 18:\tcost = 0.443\n",
      "iteration 19:\tcost = 0.438\n",
      "iteration 20:\tcost = 0.433\n",
      "{'AUC': 0.5223214285714286,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.9553571428571429,\n",
      " 'error_test': 0.214,\n",
      " 'error_train': 0.154}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 9,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.845\n",
      "iteration 3:\tcost = 0.735\n",
      "iteration 4:\tcost = 0.655\n",
      "iteration 5:\tcost = 0.598\n",
      "iteration 6:\tcost = 0.556\n",
      "iteration 7:\tcost = 0.524\n",
      "iteration 8:\tcost = 0.500\n",
      "iteration 9:\tcost = 0.481\n",
      "iteration 10:\tcost = 0.466\n",
      "iteration 11:\tcost = 0.453\n",
      "iteration 12:\tcost = 0.443\n",
      "iteration 13:\tcost = 0.434\n",
      "iteration 14:\tcost = 0.426\n",
      "iteration 15:\tcost = 0.419\n",
      "iteration 16:\tcost = 0.412\n",
      "iteration 17:\tcost = 0.406\n",
      "iteration 18:\tcost = 0.401\n",
      "iteration 19:\tcost = 0.395\n",
      "iteration 20:\tcost = 0.390\n",
      "{'AUC': 0.51376146788990829,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.97247706422018354,\n",
      " 'error_test': 0.21199999999999999,\n",
      " 'error_train': 0.13600000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 10,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.857\n",
      "iteration 3:\tcost = 0.755\n",
      "iteration 4:\tcost = 0.681\n",
      "iteration 5:\tcost = 0.628\n",
      "iteration 6:\tcost = 0.588\n",
      "iteration 7:\tcost = 0.558\n",
      "iteration 8:\tcost = 0.535\n",
      "iteration 9:\tcost = 0.517\n",
      "iteration 10:\tcost = 0.503\n",
      "iteration 11:\tcost = 0.491\n",
      "iteration 12:\tcost = 0.480\n",
      "iteration 13:\tcost = 0.472\n",
      "iteration 14:\tcost = 0.464\n",
      "iteration 15:\tcost = 0.457\n",
      "iteration 16:\tcost = 0.450\n",
      "iteration 17:\tcost = 0.444\n",
      "iteration 18:\tcost = 0.439\n",
      "iteration 19:\tcost = 0.434\n",
      "iteration 20:\tcost = 0.429\n",
      "{'AUC': 0.52777777777777779,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.94444444444444442,\n",
      " 'error_test': 0.20399999999999999,\n",
      " 'error_train': 0.14999999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 1,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.843\n",
      "iteration 3:\tcost = 0.730\n",
      "iteration 4:\tcost = 0.650\n",
      "iteration 5:\tcost = 0.591\n",
      "iteration 6:\tcost = 0.548\n",
      "iteration 7:\tcost = 0.516\n",
      "iteration 8:\tcost = 0.491\n",
      "iteration 9:\tcost = 0.472\n",
      "iteration 10:\tcost = 0.457\n",
      "iteration 11:\tcost = 0.444\n",
      "iteration 12:\tcost = 0.434\n",
      "iteration 13:\tcost = 0.425\n",
      "iteration 14:\tcost = 0.417\n",
      "iteration 15:\tcost = 0.410\n",
      "iteration 16:\tcost = 0.404\n",
      "iteration 17:\tcost = 0.398\n",
      "iteration 18:\tcost = 0.393\n",
      "iteration 19:\tcost = 0.388\n",
      "iteration 20:\tcost = 0.384\n",
      "{'AUC': 0.50442477876106195,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.99115044247787609,\n",
      " 'error_test': 0.224,\n",
      " 'error_train': 0.13}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 2,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.833\n",
      "iteration 3:\tcost = 0.716\n",
      "iteration 4:\tcost = 0.632\n",
      "iteration 5:\tcost = 0.573\n",
      "iteration 6:\tcost = 0.530\n",
      "iteration 7:\tcost = 0.499\n",
      "iteration 8:\tcost = 0.476\n",
      "iteration 9:\tcost = 0.459\n",
      "iteration 10:\tcost = 0.446\n",
      "iteration 11:\tcost = 0.436\n",
      "iteration 12:\tcost = 0.428\n",
      "iteration 13:\tcost = 0.421\n",
      "iteration 14:\tcost = 0.415\n",
      "iteration 15:\tcost = 0.411\n",
      "iteration 16:\tcost = 0.406\n",
      "iteration 17:\tcost = 0.402\n",
      "iteration 18:\tcost = 0.399\n",
      "iteration 19:\tcost = 0.396\n",
      "iteration 20:\tcost = 0.393\n",
      "{'AUC': 0.5,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 1.0,\n",
      " 'error_test': 0.20200000000000001,\n",
      " 'error_train': 0.128}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 3,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.823\n",
      "iteration 3:\tcost = 0.697\n",
      "iteration 4:\tcost = 0.608\n",
      "iteration 5:\tcost = 0.544\n",
      "iteration 6:\tcost = 0.497\n",
      "iteration 7:\tcost = 0.463\n",
      "iteration 8:\tcost = 0.438\n",
      "iteration 9:\tcost = 0.419\n",
      "iteration 10:\tcost = 0.404\n",
      "iteration 11:\tcost = 0.392\n",
      "iteration 12:\tcost = 0.382\n",
      "iteration 13:\tcost = 0.374\n",
      "iteration 14:\tcost = 0.367\n",
      "iteration 15:\tcost = 0.360\n",
      "iteration 16:\tcost = 0.355\n",
      "iteration 17:\tcost = 0.350\n",
      "iteration 18:\tcost = 0.345\n",
      "iteration 19:\tcost = 0.341\n",
      "iteration 20:\tcost = 0.336\n",
      "{'AUC': 0.51339285714285721,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.9732142857142857,\n",
      " 'error_test': 0.218,\n",
      " 'error_train': 0.11799999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 4,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.824\n",
      "iteration 3:\tcost = 0.700\n",
      "iteration 4:\tcost = 0.612\n",
      "iteration 5:\tcost = 0.549\n",
      "iteration 6:\tcost = 0.503\n",
      "iteration 7:\tcost = 0.470\n",
      "iteration 8:\tcost = 0.445\n",
      "iteration 9:\tcost = 0.427\n",
      "iteration 10:\tcost = 0.412\n",
      "iteration 11:\tcost = 0.401\n",
      "iteration 12:\tcost = 0.392\n",
      "iteration 13:\tcost = 0.384\n",
      "iteration 14:\tcost = 0.378\n",
      "iteration 15:\tcost = 0.372\n",
      "iteration 16:\tcost = 0.367\n",
      "iteration 17:\tcost = 0.362\n",
      "iteration 18:\tcost = 0.358\n",
      "iteration 19:\tcost = 0.354\n",
      "iteration 20:\tcost = 0.350\n",
      "{'AUC': 0.5092592592592593,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.98148148148148151,\n",
      " 'error_test': 0.21199999999999999,\n",
      " 'error_train': 0.114}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 5,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.832\n",
      "iteration 3:\tcost = 0.713\n",
      "iteration 4:\tcost = 0.627\n",
      "iteration 5:\tcost = 0.565\n",
      "iteration 6:\tcost = 0.520\n",
      "iteration 7:\tcost = 0.487\n",
      "iteration 8:\tcost = 0.462\n",
      "iteration 9:\tcost = 0.443\n",
      "iteration 10:\tcost = 0.427\n",
      "iteration 11:\tcost = 0.415\n",
      "iteration 12:\tcost = 0.405\n",
      "iteration 13:\tcost = 0.396\n",
      "iteration 14:\tcost = 0.389\n",
      "iteration 15:\tcost = 0.382\n",
      "iteration 16:\tcost = 0.376\n",
      "iteration 17:\tcost = 0.370\n",
      "iteration 18:\tcost = 0.365\n",
      "iteration 19:\tcost = 0.361\n",
      "iteration 20:\tcost = 0.356\n",
      "{'AUC': 0.51293103448275867,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.97413793103448276,\n",
      " 'error_test': 0.22600000000000001,\n",
      " 'error_train': 0.124}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 6,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.829\n",
      "iteration 3:\tcost = 0.708\n",
      "iteration 4:\tcost = 0.622\n",
      "iteration 5:\tcost = 0.560\n",
      "iteration 6:\tcost = 0.515\n",
      "iteration 7:\tcost = 0.482\n",
      "iteration 8:\tcost = 0.458\n",
      "iteration 9:\tcost = 0.439\n",
      "iteration 10:\tcost = 0.424\n",
      "iteration 11:\tcost = 0.413\n",
      "iteration 12:\tcost = 0.403\n",
      "iteration 13:\tcost = 0.395\n",
      "iteration 14:\tcost = 0.388\n",
      "iteration 15:\tcost = 0.382\n",
      "iteration 16:\tcost = 0.377\n",
      "iteration 17:\tcost = 0.372\n",
      "iteration 18:\tcost = 0.368\n",
      "iteration 19:\tcost = 0.363\n",
      "iteration 20:\tcost = 0.360\n",
      "{'AUC': 0.50471698113207553,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.99056603773584906,\n",
      " 'error_test': 0.20999999999999999,\n",
      " 'error_train': 0.11799999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 7,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.836\n",
      "iteration 3:\tcost = 0.720\n",
      "iteration 4:\tcost = 0.637\n",
      "iteration 5:\tcost = 0.578\n",
      "iteration 6:\tcost = 0.534\n",
      "iteration 7:\tcost = 0.502\n",
      "iteration 8:\tcost = 0.478\n",
      "iteration 9:\tcost = 0.459\n",
      "iteration 10:\tcost = 0.445\n",
      "iteration 11:\tcost = 0.433\n",
      "iteration 12:\tcost = 0.423\n",
      "iteration 13:\tcost = 0.415\n",
      "iteration 14:\tcost = 0.408\n",
      "iteration 15:\tcost = 0.402\n",
      "iteration 16:\tcost = 0.396\n",
      "iteration 17:\tcost = 0.391\n",
      "iteration 18:\tcost = 0.386\n",
      "iteration 19:\tcost = 0.382\n",
      "iteration 20:\tcost = 0.377\n",
      "{'AUC': 0.51315789473684204,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.97368421052631582,\n",
      " 'error_test': 0.222,\n",
      " 'error_train': 0.13}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 8,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.836\n",
      "iteration 3:\tcost = 0.720\n",
      "iteration 4:\tcost = 0.636\n",
      "iteration 5:\tcost = 0.576\n",
      "iteration 6:\tcost = 0.533\n",
      "iteration 7:\tcost = 0.500\n",
      "iteration 8:\tcost = 0.476\n",
      "iteration 9:\tcost = 0.457\n",
      "iteration 10:\tcost = 0.443\n",
      "iteration 11:\tcost = 0.431\n",
      "iteration 12:\tcost = 0.421\n",
      "iteration 13:\tcost = 0.413\n",
      "iteration 14:\tcost = 0.406\n",
      "iteration 15:\tcost = 0.400\n",
      "iteration 16:\tcost = 0.394\n",
      "iteration 17:\tcost = 0.389\n",
      "iteration 18:\tcost = 0.384\n",
      "iteration 19:\tcost = 0.380\n",
      "iteration 20:\tcost = 0.376\n",
      "{'AUC': 0.5092592592592593,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.98148148148148151,\n",
      " 'error_test': 0.21199999999999999,\n",
      " 'error_train': 0.124}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 9,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.851\n",
      "iteration 3:\tcost = 0.744\n",
      "iteration 4:\tcost = 0.667\n",
      "iteration 5:\tcost = 0.612\n",
      "iteration 6:\tcost = 0.571\n",
      "iteration 7:\tcost = 0.540\n",
      "iteration 8:\tcost = 0.516\n",
      "iteration 9:\tcost = 0.498\n",
      "iteration 10:\tcost = 0.483\n",
      "iteration 11:\tcost = 0.471\n",
      "iteration 12:\tcost = 0.461\n",
      "iteration 13:\tcost = 0.452\n",
      "iteration 14:\tcost = 0.444\n",
      "iteration 15:\tcost = 0.437\n",
      "iteration 16:\tcost = 0.431\n",
      "iteration 17:\tcost = 0.425\n",
      "iteration 18:\tcost = 0.420\n",
      "iteration 19:\tcost = 0.414\n",
      "iteration 20:\tcost = 0.409\n",
      "{'AUC': 0.53125,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.9375,\n",
      " 'error_test': 0.17999999999999999,\n",
      " 'error_train': 0.14799999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 10,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.840\n",
      "iteration 3:\tcost = 0.726\n",
      "iteration 4:\tcost = 0.644\n",
      "iteration 5:\tcost = 0.584\n",
      "iteration 6:\tcost = 0.539\n",
      "iteration 7:\tcost = 0.505\n",
      "iteration 8:\tcost = 0.479\n",
      "iteration 9:\tcost = 0.459\n",
      "iteration 10:\tcost = 0.442\n",
      "iteration 11:\tcost = 0.429\n",
      "iteration 12:\tcost = 0.417\n",
      "iteration 13:\tcost = 0.407\n",
      "iteration 14:\tcost = 0.399\n",
      "iteration 15:\tcost = 0.391\n",
      "iteration 16:\tcost = 0.384\n",
      "iteration 17:\tcost = 0.377\n",
      "iteration 18:\tcost = 0.371\n",
      "iteration 19:\tcost = 0.366\n",
      "iteration 20:\tcost = 0.360\n",
      "{'AUC': 0.50442477876106195,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.99115044247787609,\n",
      " 'error_test': 0.224,\n",
      " 'error_train': 0.13}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 1,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.886\n",
      "iteration 3:\tcost = 0.804\n",
      "iteration 4:\tcost = 0.744\n",
      "iteration 5:\tcost = 0.700\n",
      "iteration 6:\tcost = 0.667\n",
      "iteration 7:\tcost = 0.641\n",
      "iteration 8:\tcost = 0.621\n",
      "iteration 9:\tcost = 0.604\n",
      "iteration 10:\tcost = 0.590\n",
      "iteration 11:\tcost = 0.578\n",
      "iteration 12:\tcost = 0.567\n",
      "iteration 13:\tcost = 0.558\n",
      "iteration 14:\tcost = 0.549\n",
      "iteration 15:\tcost = 0.542\n",
      "iteration 16:\tcost = 0.534\n",
      "iteration 17:\tcost = 0.528\n",
      "iteration 18:\tcost = 0.521\n",
      "iteration 19:\tcost = 0.516\n",
      "iteration 20:\tcost = 0.510\n",
      "{'AUC': 0.56956521739130439,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.86086956521739133,\n",
      " 'error_test': 0.19800000000000001,\n",
      " 'error_train': 0.17000000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 2,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.889\n",
      "iteration 3:\tcost = 0.809\n",
      "iteration 4:\tcost = 0.750\n",
      "iteration 5:\tcost = 0.706\n",
      "iteration 6:\tcost = 0.672\n",
      "iteration 7:\tcost = 0.646\n",
      "iteration 8:\tcost = 0.624\n",
      "iteration 9:\tcost = 0.607\n",
      "iteration 10:\tcost = 0.592\n",
      "iteration 11:\tcost = 0.579\n",
      "iteration 12:\tcost = 0.567\n",
      "iteration 13:\tcost = 0.557\n",
      "iteration 14:\tcost = 0.548\n",
      "iteration 15:\tcost = 0.539\n",
      "iteration 16:\tcost = 0.531\n",
      "iteration 17:\tcost = 0.524\n",
      "iteration 18:\tcost = 0.517\n",
      "iteration 19:\tcost = 0.511\n",
      "iteration 20:\tcost = 0.505\n",
      "{'AUC': 0.58035714285714279,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.8392857142857143,\n",
      " 'error_test': 0.188,\n",
      " 'error_train': 0.17000000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 3,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.895\n",
      "iteration 3:\tcost = 0.819\n",
      "iteration 4:\tcost = 0.762\n",
      "iteration 5:\tcost = 0.718\n",
      "iteration 6:\tcost = 0.684\n",
      "iteration 7:\tcost = 0.657\n",
      "iteration 8:\tcost = 0.635\n",
      "iteration 9:\tcost = 0.616\n",
      "iteration 10:\tcost = 0.600\n",
      "iteration 11:\tcost = 0.585\n",
      "iteration 12:\tcost = 0.573\n",
      "iteration 13:\tcost = 0.561\n",
      "iteration 14:\tcost = 0.551\n",
      "iteration 15:\tcost = 0.542\n",
      "iteration 16:\tcost = 0.533\n",
      "iteration 17:\tcost = 0.525\n",
      "iteration 18:\tcost = 0.517\n",
      "iteration 19:\tcost = 0.510\n",
      "iteration 20:\tcost = 0.504\n",
      "{'AUC': 0.58018867924528306,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.839622641509434,\n",
      " 'error_test': 0.17799999999999999,\n",
      " 'error_train': 0.154}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 4,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.901\n",
      "iteration 3:\tcost = 0.830\n",
      "iteration 4:\tcost = 0.778\n",
      "iteration 5:\tcost = 0.739\n",
      "iteration 6:\tcost = 0.709\n",
      "iteration 7:\tcost = 0.686\n",
      "iteration 8:\tcost = 0.667\n",
      "iteration 9:\tcost = 0.651\n",
      "iteration 10:\tcost = 0.638\n",
      "iteration 11:\tcost = 0.627\n",
      "iteration 12:\tcost = 0.617\n",
      "iteration 13:\tcost = 0.608\n",
      "iteration 14:\tcost = 0.600\n",
      "iteration 15:\tcost = 0.593\n",
      "iteration 16:\tcost = 0.586\n",
      "iteration 17:\tcost = 0.580\n",
      "iteration 18:\tcost = 0.574\n",
      "iteration 19:\tcost = 0.569\n",
      "iteration 20:\tcost = 0.564\n",
      "{'AUC': 0.56730769230769229,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.86538461538461542,\n",
      " 'error_test': 0.17999999999999999,\n",
      " 'error_train': 0.184}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 5,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.901\n",
      "iteration 3:\tcost = 0.828\n",
      "iteration 4:\tcost = 0.775\n",
      "iteration 5:\tcost = 0.733\n",
      "iteration 6:\tcost = 0.701\n",
      "iteration 7:\tcost = 0.675\n",
      "iteration 8:\tcost = 0.654\n",
      "iteration 9:\tcost = 0.636\n",
      "iteration 10:\tcost = 0.621\n",
      "iteration 11:\tcost = 0.607\n",
      "iteration 12:\tcost = 0.595\n",
      "iteration 13:\tcost = 0.584\n",
      "iteration 14:\tcost = 0.574\n",
      "iteration 15:\tcost = 0.564\n",
      "iteration 16:\tcost = 0.556\n",
      "iteration 17:\tcost = 0.548\n",
      "iteration 18:\tcost = 0.540\n",
      "iteration 19:\tcost = 0.533\n",
      "iteration 20:\tcost = 0.526\n",
      "{'AUC': 0.61639077741649428,\n",
      " 'FNR': 0.0025125628140703518,\n",
      " 'FPR': 0.76470588235294112,\n",
      " 'error_test': 0.158,\n",
      " 'error_train': 0.182}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 6,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.883\n",
      "iteration 3:\tcost = 0.799\n",
      "iteration 4:\tcost = 0.737\n",
      "iteration 5:\tcost = 0.691\n",
      "iteration 6:\tcost = 0.656\n",
      "iteration 7:\tcost = 0.628\n",
      "iteration 8:\tcost = 0.605\n",
      "iteration 9:\tcost = 0.587\n",
      "iteration 10:\tcost = 0.571\n",
      "iteration 11:\tcost = 0.558\n",
      "iteration 12:\tcost = 0.545\n",
      "iteration 13:\tcost = 0.535\n",
      "iteration 14:\tcost = 0.525\n",
      "iteration 15:\tcost = 0.516\n",
      "iteration 16:\tcost = 0.508\n",
      "iteration 17:\tcost = 0.500\n",
      "iteration 18:\tcost = 0.493\n",
      "iteration 19:\tcost = 0.486\n",
      "iteration 20:\tcost = 0.480\n",
      "{'AUC': 0.57692307692307687,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.84615384615384615,\n",
      " 'error_test': 0.19800000000000001,\n",
      " 'error_train': 0.16800000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 7,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.909\n",
      "iteration 3:\tcost = 0.842\n",
      "iteration 4:\tcost = 0.791\n",
      "iteration 5:\tcost = 0.752\n",
      "iteration 6:\tcost = 0.721\n",
      "iteration 7:\tcost = 0.695\n",
      "iteration 8:\tcost = 0.674\n",
      "iteration 9:\tcost = 0.656\n",
      "iteration 10:\tcost = 0.640\n",
      "iteration 11:\tcost = 0.625\n",
      "iteration 12:\tcost = 0.612\n",
      "iteration 13:\tcost = 0.601\n",
      "iteration 14:\tcost = 0.590\n",
      "iteration 15:\tcost = 0.580\n",
      "iteration 16:\tcost = 0.571\n",
      "iteration 17:\tcost = 0.563\n",
      "iteration 18:\tcost = 0.555\n",
      "iteration 19:\tcost = 0.547\n",
      "iteration 20:\tcost = 0.540\n",
      "{'AUC': 0.62631578947368416,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.74736842105263157,\n",
      " 'error_test': 0.14199999999999999,\n",
      " 'error_train': 0.17799999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 8,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.902\n",
      "iteration 3:\tcost = 0.830\n",
      "iteration 4:\tcost = 0.777\n",
      "iteration 5:\tcost = 0.736\n",
      "iteration 6:\tcost = 0.704\n",
      "iteration 7:\tcost = 0.679\n",
      "iteration 8:\tcost = 0.658\n",
      "iteration 9:\tcost = 0.640\n",
      "iteration 10:\tcost = 0.625\n",
      "iteration 11:\tcost = 0.611\n",
      "iteration 12:\tcost = 0.599\n",
      "iteration 13:\tcost = 0.589\n",
      "iteration 14:\tcost = 0.579\n",
      "iteration 15:\tcost = 0.570\n",
      "iteration 16:\tcost = 0.561\n",
      "iteration 17:\tcost = 0.554\n",
      "iteration 18:\tcost = 0.546\n",
      "iteration 19:\tcost = 0.540\n",
      "iteration 20:\tcost = 0.534\n",
      "{'AUC': 0.60194174757281549,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.79611650485436891,\n",
      " 'error_test': 0.16400000000000001,\n",
      " 'error_train': 0.16600000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 9,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.895\n",
      "iteration 3:\tcost = 0.820\n",
      "iteration 4:\tcost = 0.764\n",
      "iteration 5:\tcost = 0.722\n",
      "iteration 6:\tcost = 0.690\n",
      "iteration 7:\tcost = 0.665\n",
      "iteration 8:\tcost = 0.644\n",
      "iteration 9:\tcost = 0.628\n",
      "iteration 10:\tcost = 0.613\n",
      "iteration 11:\tcost = 0.601\n",
      "iteration 12:\tcost = 0.589\n",
      "iteration 13:\tcost = 0.580\n",
      "iteration 14:\tcost = 0.571\n",
      "iteration 15:\tcost = 0.562\n",
      "iteration 16:\tcost = 0.555\n",
      "iteration 17:\tcost = 0.548\n",
      "iteration 18:\tcost = 0.541\n",
      "iteration 19:\tcost = 0.535\n",
      "iteration 20:\tcost = 0.529\n",
      "{'AUC': 0.59722222222222221,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.80555555555555558,\n",
      " 'error_test': 0.17399999999999999,\n",
      " 'error_train': 0.17599999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 10,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.890\n",
      "iteration 3:\tcost = 0.812\n",
      "iteration 4:\tcost = 0.755\n",
      "iteration 5:\tcost = 0.713\n",
      "iteration 6:\tcost = 0.681\n",
      "iteration 7:\tcost = 0.657\n",
      "iteration 8:\tcost = 0.637\n",
      "iteration 9:\tcost = 0.622\n",
      "iteration 10:\tcost = 0.609\n",
      "iteration 11:\tcost = 0.597\n",
      "iteration 12:\tcost = 0.588\n",
      "iteration 13:\tcost = 0.579\n",
      "iteration 14:\tcost = 0.572\n",
      "iteration 15:\tcost = 0.565\n",
      "iteration 16:\tcost = 0.558\n",
      "iteration 17:\tcost = 0.552\n",
      "iteration 18:\tcost = 0.547\n",
      "iteration 19:\tcost = 0.541\n",
      "iteration 20:\tcost = 0.536\n",
      "{'AUC': 0.5663716814159292,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.86725663716814161,\n",
      " 'error_test': 0.19600000000000001,\n",
      " 'error_train': 0.184}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 1,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.886\n",
      "iteration 3:\tcost = 0.804\n",
      "iteration 4:\tcost = 0.744\n",
      "iteration 5:\tcost = 0.699\n",
      "iteration 6:\tcost = 0.665\n",
      "iteration 7:\tcost = 0.638\n",
      "iteration 8:\tcost = 0.617\n",
      "iteration 9:\tcost = 0.599\n",
      "iteration 10:\tcost = 0.584\n",
      "iteration 11:\tcost = 0.571\n",
      "iteration 12:\tcost = 0.560\n",
      "iteration 13:\tcost = 0.549\n",
      "iteration 14:\tcost = 0.540\n",
      "iteration 15:\tcost = 0.532\n",
      "iteration 16:\tcost = 0.524\n",
      "iteration 17:\tcost = 0.517\n",
      "iteration 18:\tcost = 0.510\n",
      "iteration 19:\tcost = 0.504\n",
      "iteration 20:\tcost = 0.499\n",
      "{'AUC': 0.59722222222222221,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.80555555555555558,\n",
      " 'error_test': 0.17399999999999999,\n",
      " 'error_train': 0.17399999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 2,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.883\n",
      "iteration 3:\tcost = 0.799\n",
      "iteration 4:\tcost = 0.738\n",
      "iteration 5:\tcost = 0.692\n",
      "iteration 6:\tcost = 0.658\n",
      "iteration 7:\tcost = 0.632\n",
      "iteration 8:\tcost = 0.611\n",
      "iteration 9:\tcost = 0.594\n",
      "iteration 10:\tcost = 0.580\n",
      "iteration 11:\tcost = 0.568\n",
      "iteration 12:\tcost = 0.558\n",
      "iteration 13:\tcost = 0.548\n",
      "iteration 14:\tcost = 0.540\n",
      "iteration 15:\tcost = 0.532\n",
      "iteration 16:\tcost = 0.525\n",
      "iteration 17:\tcost = 0.519\n",
      "iteration 18:\tcost = 0.512\n",
      "iteration 19:\tcost = 0.507\n",
      "iteration 20:\tcost = 0.501\n",
      "{'AUC': 0.54910714285714279,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.9017857142857143,\n",
      " 'error_test': 0.20200000000000001,\n",
      " 'error_train': 0.17000000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 3,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.884\n",
      "iteration 3:\tcost = 0.800\n",
      "iteration 4:\tcost = 0.739\n",
      "iteration 5:\tcost = 0.694\n",
      "iteration 6:\tcost = 0.660\n",
      "iteration 7:\tcost = 0.634\n",
      "iteration 8:\tcost = 0.613\n",
      "iteration 9:\tcost = 0.596\n",
      "iteration 10:\tcost = 0.582\n",
      "iteration 11:\tcost = 0.570\n",
      "iteration 12:\tcost = 0.559\n",
      "iteration 13:\tcost = 0.550\n",
      "iteration 14:\tcost = 0.542\n",
      "iteration 15:\tcost = 0.534\n",
      "iteration 16:\tcost = 0.527\n",
      "iteration 17:\tcost = 0.521\n",
      "iteration 18:\tcost = 0.515\n",
      "iteration 19:\tcost = 0.510\n",
      "iteration 20:\tcost = 0.504\n",
      "{'AUC': 0.54629629629629628,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.90740740740740744,\n",
      " 'error_test': 0.19600000000000001,\n",
      " 'error_train': 0.16800000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 4,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.878\n",
      "iteration 3:\tcost = 0.791\n",
      "iteration 4:\tcost = 0.728\n",
      "iteration 5:\tcost = 0.681\n",
      "iteration 6:\tcost = 0.646\n",
      "iteration 7:\tcost = 0.618\n",
      "iteration 8:\tcost = 0.597\n",
      "iteration 9:\tcost = 0.579\n",
      "iteration 10:\tcost = 0.564\n",
      "iteration 11:\tcost = 0.552\n",
      "iteration 12:\tcost = 0.540\n",
      "iteration 13:\tcost = 0.531\n",
      "iteration 14:\tcost = 0.522\n",
      "iteration 15:\tcost = 0.514\n",
      "iteration 16:\tcost = 0.506\n",
      "iteration 17:\tcost = 0.499\n",
      "iteration 18:\tcost = 0.492\n",
      "iteration 19:\tcost = 0.486\n",
      "iteration 20:\tcost = 0.481\n",
      "{'AUC': 0.56481481481481488,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.87037037037037035,\n",
      " 'error_test': 0.188,\n",
      " 'error_train': 0.16800000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 5,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.887\n",
      "iteration 3:\tcost = 0.807\n",
      "iteration 4:\tcost = 0.749\n",
      "iteration 5:\tcost = 0.707\n",
      "iteration 6:\tcost = 0.675\n",
      "iteration 7:\tcost = 0.651\n",
      "iteration 8:\tcost = 0.633\n",
      "iteration 9:\tcost = 0.618\n",
      "iteration 10:\tcost = 0.605\n",
      "iteration 11:\tcost = 0.595\n",
      "iteration 12:\tcost = 0.586\n",
      "iteration 13:\tcost = 0.578\n",
      "iteration 14:\tcost = 0.571\n",
      "iteration 15:\tcost = 0.564\n",
      "iteration 16:\tcost = 0.558\n",
      "iteration 17:\tcost = 0.553\n",
      "iteration 18:\tcost = 0.548\n",
      "iteration 19:\tcost = 0.543\n",
      "iteration 20:\tcost = 0.538\n",
      "{'AUC': 0.54326923076923084,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.91346153846153844,\n",
      " 'error_test': 0.19,\n",
      " 'error_train': 0.188}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 6,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.873\n",
      "iteration 3:\tcost = 0.782\n",
      "iteration 4:\tcost = 0.716\n",
      "iteration 5:\tcost = 0.667\n",
      "iteration 6:\tcost = 0.630\n",
      "iteration 7:\tcost = 0.602\n",
      "iteration 8:\tcost = 0.579\n",
      "iteration 9:\tcost = 0.561\n",
      "iteration 10:\tcost = 0.546\n",
      "iteration 11:\tcost = 0.533\n",
      "iteration 12:\tcost = 0.522\n",
      "iteration 13:\tcost = 0.512\n",
      "iteration 14:\tcost = 0.503\n",
      "iteration 15:\tcost = 0.495\n",
      "iteration 16:\tcost = 0.488\n",
      "iteration 17:\tcost = 0.481\n",
      "iteration 18:\tcost = 0.474\n",
      "iteration 19:\tcost = 0.468\n",
      "iteration 20:\tcost = 0.463\n",
      "{'AUC': 0.55607476635514019,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.88785046728971961,\n",
      " 'error_test': 0.19,\n",
      " 'error_train': 0.154}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 7,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.888\n",
      "iteration 3:\tcost = 0.808\n",
      "iteration 4:\tcost = 0.749\n",
      "iteration 5:\tcost = 0.706\n",
      "iteration 6:\tcost = 0.672\n",
      "iteration 7:\tcost = 0.646\n",
      "iteration 8:\tcost = 0.626\n",
      "iteration 9:\tcost = 0.609\n",
      "iteration 10:\tcost = 0.594\n",
      "iteration 11:\tcost = 0.582\n",
      "iteration 12:\tcost = 0.571\n",
      "iteration 13:\tcost = 0.562\n",
      "iteration 14:\tcost = 0.553\n",
      "iteration 15:\tcost = 0.545\n",
      "iteration 16:\tcost = 0.538\n",
      "iteration 17:\tcost = 0.531\n",
      "iteration 18:\tcost = 0.525\n",
      "iteration 19:\tcost = 0.519\n",
      "iteration 20:\tcost = 0.514\n",
      "{'AUC': 0.56666666666666665,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.8666666666666667,\n",
      " 'error_test': 0.182,\n",
      " 'error_train': 0.17599999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 8,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.892\n",
      "iteration 3:\tcost = 0.815\n",
      "iteration 4:\tcost = 0.757\n",
      "iteration 5:\tcost = 0.714\n",
      "iteration 6:\tcost = 0.681\n",
      "iteration 7:\tcost = 0.655\n",
      "iteration 8:\tcost = 0.634\n",
      "iteration 9:\tcost = 0.617\n",
      "iteration 10:\tcost = 0.602\n",
      "iteration 11:\tcost = 0.589\n",
      "iteration 12:\tcost = 0.578\n",
      "iteration 13:\tcost = 0.568\n",
      "iteration 14:\tcost = 0.558\n",
      "iteration 15:\tcost = 0.550\n",
      "iteration 16:\tcost = 0.542\n",
      "iteration 17:\tcost = 0.535\n",
      "iteration 18:\tcost = 0.529\n",
      "iteration 19:\tcost = 0.523\n",
      "iteration 20:\tcost = 0.517\n",
      "{'AUC': 0.57766990291262132,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.84466019417475724,\n",
      " 'error_test': 0.17399999999999999,\n",
      " 'error_train': 0.17199999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 9,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.866\n",
      "iteration 3:\tcost = 0.772\n",
      "iteration 4:\tcost = 0.703\n",
      "iteration 5:\tcost = 0.654\n",
      "iteration 6:\tcost = 0.617\n",
      "iteration 7:\tcost = 0.590\n",
      "iteration 8:\tcost = 0.568\n",
      "iteration 9:\tcost = 0.552\n",
      "iteration 10:\tcost = 0.538\n",
      "iteration 11:\tcost = 0.526\n",
      "iteration 12:\tcost = 0.517\n",
      "iteration 13:\tcost = 0.508\n",
      "iteration 14:\tcost = 0.501\n",
      "iteration 15:\tcost = 0.494\n",
      "iteration 16:\tcost = 0.488\n",
      "iteration 17:\tcost = 0.483\n",
      "iteration 18:\tcost = 0.477\n",
      "iteration 19:\tcost = 0.473\n",
      "iteration 20:\tcost = 0.468\n",
      "{'AUC': 0.51666666666666661,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.96666666666666667,\n",
      " 'error_test': 0.23200000000000001,\n",
      " 'error_train': 0.16200000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 10,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.890\n",
      "iteration 3:\tcost = 0.811\n",
      "iteration 4:\tcost = 0.755\n",
      "iteration 5:\tcost = 0.713\n",
      "iteration 6:\tcost = 0.682\n",
      "iteration 7:\tcost = 0.659\n",
      "iteration 8:\tcost = 0.640\n",
      "iteration 9:\tcost = 0.626\n",
      "iteration 10:\tcost = 0.613\n",
      "iteration 11:\tcost = 0.603\n",
      "iteration 12:\tcost = 0.594\n",
      "iteration 13:\tcost = 0.586\n",
      "iteration 14:\tcost = 0.579\n",
      "iteration 15:\tcost = 0.572\n",
      "iteration 16:\tcost = 0.566\n",
      "iteration 17:\tcost = 0.561\n",
      "iteration 18:\tcost = 0.555\n",
      "iteration 19:\tcost = 0.550\n",
      "iteration 20:\tcost = 0.546\n",
      "{'AUC': 0.55188679245283012,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.89622641509433965,\n",
      " 'error_test': 0.19,\n",
      " 'error_train': 0.186}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 1,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.867\n",
      "iteration 3:\tcost = 0.772\n",
      "iteration 4:\tcost = 0.703\n",
      "iteration 5:\tcost = 0.652\n",
      "iteration 6:\tcost = 0.614\n",
      "iteration 7:\tcost = 0.585\n",
      "iteration 8:\tcost = 0.563\n",
      "iteration 9:\tcost = 0.545\n",
      "iteration 10:\tcost = 0.530\n",
      "iteration 11:\tcost = 0.517\n",
      "iteration 12:\tcost = 0.506\n",
      "iteration 13:\tcost = 0.496\n",
      "iteration 14:\tcost = 0.488\n",
      "iteration 15:\tcost = 0.480\n",
      "iteration 16:\tcost = 0.473\n",
      "iteration 17:\tcost = 0.466\n",
      "iteration 18:\tcost = 0.460\n",
      "iteration 19:\tcost = 0.454\n",
      "iteration 20:\tcost = 0.449\n",
      "{'AUC': 0.5347826086956522,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.93043478260869561,\n",
      " 'error_test': 0.214,\n",
      " 'error_train': 0.14999999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 2,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.875\n",
      "iteration 3:\tcost = 0.785\n",
      "iteration 4:\tcost = 0.720\n",
      "iteration 5:\tcost = 0.672\n",
      "iteration 6:\tcost = 0.636\n",
      "iteration 7:\tcost = 0.608\n",
      "iteration 8:\tcost = 0.586\n",
      "iteration 9:\tcost = 0.568\n",
      "iteration 10:\tcost = 0.553\n",
      "iteration 11:\tcost = 0.540\n",
      "iteration 12:\tcost = 0.529\n",
      "iteration 13:\tcost = 0.520\n",
      "iteration 14:\tcost = 0.511\n",
      "iteration 15:\tcost = 0.503\n",
      "iteration 16:\tcost = 0.496\n",
      "iteration 17:\tcost = 0.489\n",
      "iteration 18:\tcost = 0.483\n",
      "iteration 19:\tcost = 0.477\n",
      "iteration 20:\tcost = 0.471\n",
      "{'AUC': 0.5420560747663552,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.91588785046728971,\n",
      " 'error_test': 0.19600000000000001,\n",
      " 'error_train': 0.16}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 3,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.870\n",
      "iteration 3:\tcost = 0.777\n",
      "iteration 4:\tcost = 0.710\n",
      "iteration 5:\tcost = 0.662\n",
      "iteration 6:\tcost = 0.627\n",
      "iteration 7:\tcost = 0.600\n",
      "iteration 8:\tcost = 0.579\n",
      "iteration 9:\tcost = 0.563\n",
      "iteration 10:\tcost = 0.550\n",
      "iteration 11:\tcost = 0.539\n",
      "iteration 12:\tcost = 0.530\n",
      "iteration 13:\tcost = 0.521\n",
      "iteration 14:\tcost = 0.514\n",
      "iteration 15:\tcost = 0.508\n",
      "iteration 16:\tcost = 0.502\n",
      "iteration 17:\tcost = 0.496\n",
      "iteration 18:\tcost = 0.491\n",
      "iteration 19:\tcost = 0.486\n",
      "iteration 20:\tcost = 0.481\n",
      "{'AUC': 0.53153153153153154,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.93693693693693691,\n",
      " 'error_test': 0.20799999999999999,\n",
      " 'error_train': 0.17000000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 4,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.856\n",
      "iteration 3:\tcost = 0.754\n",
      "iteration 4:\tcost = 0.681\n",
      "iteration 5:\tcost = 0.628\n",
      "iteration 6:\tcost = 0.589\n",
      "iteration 7:\tcost = 0.560\n",
      "iteration 8:\tcost = 0.538\n",
      "iteration 9:\tcost = 0.521\n",
      "iteration 10:\tcost = 0.507\n",
      "iteration 11:\tcost = 0.495\n",
      "iteration 12:\tcost = 0.486\n",
      "iteration 13:\tcost = 0.477\n",
      "iteration 14:\tcost = 0.470\n",
      "iteration 15:\tcost = 0.463\n",
      "iteration 16:\tcost = 0.457\n",
      "iteration 17:\tcost = 0.451\n",
      "iteration 18:\tcost = 0.446\n",
      "iteration 19:\tcost = 0.441\n",
      "iteration 20:\tcost = 0.437\n",
      "{'AUC': 0.51652892561983466,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.96694214876033058,\n",
      " 'error_test': 0.23400000000000001,\n",
      " 'error_train': 0.152}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 5,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.872\n",
      "iteration 3:\tcost = 0.780\n",
      "iteration 4:\tcost = 0.714\n",
      "iteration 5:\tcost = 0.665\n",
      "iteration 6:\tcost = 0.629\n",
      "iteration 7:\tcost = 0.602\n",
      "iteration 8:\tcost = 0.580\n",
      "iteration 9:\tcost = 0.563\n",
      "iteration 10:\tcost = 0.549\n",
      "iteration 11:\tcost = 0.537\n",
      "iteration 12:\tcost = 0.526\n",
      "iteration 13:\tcost = 0.517\n",
      "iteration 14:\tcost = 0.509\n",
      "iteration 15:\tcost = 0.502\n",
      "iteration 16:\tcost = 0.495\n",
      "iteration 17:\tcost = 0.489\n",
      "iteration 18:\tcost = 0.483\n",
      "iteration 19:\tcost = 0.477\n",
      "iteration 20:\tcost = 0.472\n",
      "{'AUC': 0.5490196078431373,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.90196078431372551,\n",
      " 'error_test': 0.184,\n",
      " 'error_train': 0.16600000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 6,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.870\n",
      "iteration 3:\tcost = 0.777\n",
      "iteration 4:\tcost = 0.710\n",
      "iteration 5:\tcost = 0.661\n",
      "iteration 6:\tcost = 0.623\n",
      "iteration 7:\tcost = 0.595\n",
      "iteration 8:\tcost = 0.572\n",
      "iteration 9:\tcost = 0.554\n",
      "iteration 10:\tcost = 0.540\n",
      "iteration 11:\tcost = 0.527\n",
      "iteration 12:\tcost = 0.516\n",
      "iteration 13:\tcost = 0.506\n",
      "iteration 14:\tcost = 0.498\n",
      "iteration 15:\tcost = 0.490\n",
      "iteration 16:\tcost = 0.483\n",
      "iteration 17:\tcost = 0.476\n",
      "iteration 18:\tcost = 0.470\n",
      "iteration 19:\tcost = 0.464\n",
      "iteration 20:\tcost = 0.459\n",
      "{'AUC': 0.54545454545454541,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.90909090909090906,\n",
      " 'error_test': 0.20000000000000001,\n",
      " 'error_train': 0.16200000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 7,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.887\n",
      "iteration 3:\tcost = 0.805\n",
      "iteration 4:\tcost = 0.744\n",
      "iteration 5:\tcost = 0.699\n",
      "iteration 6:\tcost = 0.664\n",
      "iteration 7:\tcost = 0.637\n",
      "iteration 8:\tcost = 0.615\n",
      "iteration 9:\tcost = 0.597\n",
      "iteration 10:\tcost = 0.582\n",
      "iteration 11:\tcost = 0.569\n",
      "iteration 12:\tcost = 0.557\n",
      "iteration 13:\tcost = 0.547\n",
      "iteration 14:\tcost = 0.538\n",
      "iteration 15:\tcost = 0.529\n",
      "iteration 16:\tcost = 0.521\n",
      "iteration 17:\tcost = 0.514\n",
      "iteration 18:\tcost = 0.507\n",
      "iteration 19:\tcost = 0.501\n",
      "iteration 20:\tcost = 0.495\n",
      "{'AUC': 0.57425742574257432,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.85148514851485146,\n",
      " 'error_test': 0.17199999999999999,\n",
      " 'error_train': 0.17599999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 8,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.881\n",
      "iteration 3:\tcost = 0.795\n",
      "iteration 4:\tcost = 0.731\n",
      "iteration 5:\tcost = 0.683\n",
      "iteration 6:\tcost = 0.645\n",
      "iteration 7:\tcost = 0.616\n",
      "iteration 8:\tcost = 0.591\n",
      "iteration 9:\tcost = 0.571\n",
      "iteration 10:\tcost = 0.554\n",
      "iteration 11:\tcost = 0.540\n",
      "iteration 12:\tcost = 0.527\n",
      "iteration 13:\tcost = 0.515\n",
      "iteration 14:\tcost = 0.505\n",
      "iteration 15:\tcost = 0.495\n",
      "iteration 16:\tcost = 0.486\n",
      "iteration 17:\tcost = 0.478\n",
      "iteration 18:\tcost = 0.470\n",
      "iteration 19:\tcost = 0.463\n",
      "iteration 20:\tcost = 0.457\n",
      "{'AUC': 0.56930693069306937,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.86138613861386137,\n",
      " 'error_test': 0.17399999999999999,\n",
      " 'error_train': 0.14999999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 9,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.849\n",
      "iteration 3:\tcost = 0.743\n",
      "iteration 4:\tcost = 0.667\n",
      "iteration 5:\tcost = 0.613\n",
      "iteration 6:\tcost = 0.573\n",
      "iteration 7:\tcost = 0.543\n",
      "iteration 8:\tcost = 0.520\n",
      "iteration 9:\tcost = 0.503\n",
      "iteration 10:\tcost = 0.489\n",
      "iteration 11:\tcost = 0.477\n",
      "iteration 12:\tcost = 0.467\n",
      "iteration 13:\tcost = 0.459\n",
      "iteration 14:\tcost = 0.452\n",
      "iteration 15:\tcost = 0.445\n",
      "iteration 16:\tcost = 0.439\n",
      "iteration 17:\tcost = 0.433\n",
      "iteration 18:\tcost = 0.428\n",
      "iteration 19:\tcost = 0.424\n",
      "iteration 20:\tcost = 0.419\n",
      "{'AUC': 0.51260504201680668,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.97478991596638653,\n",
      " 'error_test': 0.23200000000000001,\n",
      " 'error_train': 0.14799999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 10,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.879\n",
      "iteration 3:\tcost = 0.792\n",
      "iteration 4:\tcost = 0.729\n",
      "iteration 5:\tcost = 0.683\n",
      "iteration 6:\tcost = 0.648\n",
      "iteration 7:\tcost = 0.621\n",
      "iteration 8:\tcost = 0.600\n",
      "iteration 9:\tcost = 0.584\n",
      "iteration 10:\tcost = 0.570\n",
      "iteration 11:\tcost = 0.558\n",
      "iteration 12:\tcost = 0.548\n",
      "iteration 13:\tcost = 0.539\n",
      "iteration 14:\tcost = 0.531\n",
      "iteration 15:\tcost = 0.524\n",
      "iteration 16:\tcost = 0.517\n",
      "iteration 17:\tcost = 0.511\n",
      "iteration 18:\tcost = 0.505\n",
      "iteration 19:\tcost = 0.499\n",
      "iteration 20:\tcost = 0.494\n",
      "{'AUC': 0.54285714285714293,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.91428571428571426,\n",
      " 'error_test': 0.192,\n",
      " 'error_train': 0.16800000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 1,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.866\n",
      "iteration 3:\tcost = 0.770\n",
      "iteration 4:\tcost = 0.700\n",
      "iteration 5:\tcost = 0.649\n",
      "iteration 6:\tcost = 0.610\n",
      "iteration 7:\tcost = 0.581\n",
      "iteration 8:\tcost = 0.559\n",
      "iteration 9:\tcost = 0.541\n",
      "iteration 10:\tcost = 0.526\n",
      "iteration 11:\tcost = 0.514\n",
      "iteration 12:\tcost = 0.504\n",
      "iteration 13:\tcost = 0.495\n",
      "iteration 14:\tcost = 0.487\n",
      "iteration 15:\tcost = 0.479\n",
      "iteration 16:\tcost = 0.473\n",
      "iteration 17:\tcost = 0.467\n",
      "iteration 18:\tcost = 0.461\n",
      "iteration 19:\tcost = 0.456\n",
      "iteration 20:\tcost = 0.451\n",
      "{'AUC': 0.52272727272727271,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.95454545454545459,\n",
      " 'error_test': 0.20999999999999999,\n",
      " 'error_train': 0.154}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 2,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.848\n",
      "iteration 3:\tcost = 0.740\n",
      "iteration 4:\tcost = 0.662\n",
      "iteration 5:\tcost = 0.605\n",
      "iteration 6:\tcost = 0.563\n",
      "iteration 7:\tcost = 0.532\n",
      "iteration 8:\tcost = 0.507\n",
      "iteration 9:\tcost = 0.488\n",
      "iteration 10:\tcost = 0.473\n",
      "iteration 11:\tcost = 0.460\n",
      "iteration 12:\tcost = 0.449\n",
      "iteration 13:\tcost = 0.439\n",
      "iteration 14:\tcost = 0.431\n",
      "iteration 15:\tcost = 0.424\n",
      "iteration 16:\tcost = 0.417\n",
      "iteration 17:\tcost = 0.410\n",
      "iteration 18:\tcost = 0.404\n",
      "iteration 19:\tcost = 0.399\n",
      "iteration 20:\tcost = 0.394\n",
      "{'AUC': 0.52192982456140347,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.95614035087719296,\n",
      " 'error_test': 0.218,\n",
      " 'error_train': 0.14199999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 3,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.855\n",
      "iteration 3:\tcost = 0.752\n",
      "iteration 4:\tcost = 0.678\n",
      "iteration 5:\tcost = 0.623\n",
      "iteration 6:\tcost = 0.582\n",
      "iteration 7:\tcost = 0.551\n",
      "iteration 8:\tcost = 0.527\n",
      "iteration 9:\tcost = 0.508\n",
      "iteration 10:\tcost = 0.492\n",
      "iteration 11:\tcost = 0.479\n",
      "iteration 12:\tcost = 0.468\n",
      "iteration 13:\tcost = 0.459\n",
      "iteration 14:\tcost = 0.450\n",
      "iteration 15:\tcost = 0.443\n",
      "iteration 16:\tcost = 0.436\n",
      "iteration 17:\tcost = 0.429\n",
      "iteration 18:\tcost = 0.424\n",
      "iteration 19:\tcost = 0.418\n",
      "iteration 20:\tcost = 0.413\n",
      "{'AUC': 0.51785714285714279,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.9642857142857143,\n",
      " 'error_test': 0.216,\n",
      " 'error_train': 0.14199999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 4,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.856\n",
      "iteration 3:\tcost = 0.754\n",
      "iteration 4:\tcost = 0.680\n",
      "iteration 5:\tcost = 0.625\n",
      "iteration 6:\tcost = 0.584\n",
      "iteration 7:\tcost = 0.553\n",
      "iteration 8:\tcost = 0.529\n",
      "iteration 9:\tcost = 0.510\n",
      "iteration 10:\tcost = 0.494\n",
      "iteration 11:\tcost = 0.481\n",
      "iteration 12:\tcost = 0.470\n",
      "iteration 13:\tcost = 0.460\n",
      "iteration 14:\tcost = 0.451\n",
      "iteration 15:\tcost = 0.443\n",
      "iteration 16:\tcost = 0.436\n",
      "iteration 17:\tcost = 0.429\n",
      "iteration 18:\tcost = 0.423\n",
      "iteration 19:\tcost = 0.417\n",
      "iteration 20:\tcost = 0.412\n",
      "{'AUC': 0.53240740740740744,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.93518518518518523,\n",
      " 'error_test': 0.20200000000000001,\n",
      " 'error_train': 0.14399999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 5,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.868\n",
      "iteration 3:\tcost = 0.773\n",
      "iteration 4:\tcost = 0.706\n",
      "iteration 5:\tcost = 0.656\n",
      "iteration 6:\tcost = 0.620\n",
      "iteration 7:\tcost = 0.592\n",
      "iteration 8:\tcost = 0.571\n",
      "iteration 9:\tcost = 0.555\n",
      "iteration 10:\tcost = 0.542\n",
      "iteration 11:\tcost = 0.531\n",
      "iteration 12:\tcost = 0.521\n",
      "iteration 13:\tcost = 0.513\n",
      "iteration 14:\tcost = 0.506\n",
      "iteration 15:\tcost = 0.500\n",
      "iteration 16:\tcost = 0.494\n",
      "iteration 17:\tcost = 0.489\n",
      "iteration 18:\tcost = 0.484\n",
      "iteration 19:\tcost = 0.479\n",
      "iteration 20:\tcost = 0.475\n",
      "{'AUC': 0.52314814814814814,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.95370370370370372,\n",
      " 'error_test': 0.20599999999999999,\n",
      " 'error_train': 0.16600000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 6,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.862\n",
      "iteration 3:\tcost = 0.764\n",
      "iteration 4:\tcost = 0.693\n",
      "iteration 5:\tcost = 0.641\n",
      "iteration 6:\tcost = 0.603\n",
      "iteration 7:\tcost = 0.573\n",
      "iteration 8:\tcost = 0.551\n",
      "iteration 9:\tcost = 0.533\n",
      "iteration 10:\tcost = 0.518\n",
      "iteration 11:\tcost = 0.506\n",
      "iteration 12:\tcost = 0.496\n",
      "iteration 13:\tcost = 0.487\n",
      "iteration 14:\tcost = 0.479\n",
      "iteration 15:\tcost = 0.472\n",
      "iteration 16:\tcost = 0.465\n",
      "iteration 17:\tcost = 0.459\n",
      "iteration 18:\tcost = 0.453\n",
      "iteration 19:\tcost = 0.448\n",
      "iteration 20:\tcost = 0.443\n",
      "{'AUC': 0.53271028037383172,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.93457943925233644,\n",
      " 'error_test': 0.20000000000000001,\n",
      " 'error_train': 0.156}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 7,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.866\n",
      "iteration 3:\tcost = 0.771\n",
      "iteration 4:\tcost = 0.702\n",
      "iteration 5:\tcost = 0.651\n",
      "iteration 6:\tcost = 0.614\n",
      "iteration 7:\tcost = 0.586\n",
      "iteration 8:\tcost = 0.564\n",
      "iteration 9:\tcost = 0.547\n",
      "iteration 10:\tcost = 0.533\n",
      "iteration 11:\tcost = 0.521\n",
      "iteration 12:\tcost = 0.511\n",
      "iteration 13:\tcost = 0.502\n",
      "iteration 14:\tcost = 0.494\n",
      "iteration 15:\tcost = 0.487\n",
      "iteration 16:\tcost = 0.481\n",
      "iteration 17:\tcost = 0.475\n",
      "iteration 18:\tcost = 0.469\n",
      "iteration 19:\tcost = 0.464\n",
      "iteration 20:\tcost = 0.459\n",
      "{'AUC': 0.52777777777777779,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.94444444444444442,\n",
      " 'error_test': 0.20399999999999999,\n",
      " 'error_train': 0.16200000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 8,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.844\n",
      "iteration 3:\tcost = 0.734\n",
      "iteration 4:\tcost = 0.656\n",
      "iteration 5:\tcost = 0.599\n",
      "iteration 6:\tcost = 0.558\n",
      "iteration 7:\tcost = 0.527\n",
      "iteration 8:\tcost = 0.504\n",
      "iteration 9:\tcost = 0.486\n",
      "iteration 10:\tcost = 0.472\n",
      "iteration 11:\tcost = 0.460\n",
      "iteration 12:\tcost = 0.451\n",
      "iteration 13:\tcost = 0.443\n",
      "iteration 14:\tcost = 0.435\n",
      "iteration 15:\tcost = 0.429\n",
      "iteration 16:\tcost = 0.423\n",
      "iteration 17:\tcost = 0.417\n",
      "iteration 18:\tcost = 0.412\n",
      "iteration 19:\tcost = 0.407\n",
      "iteration 20:\tcost = 0.403\n",
      "{'AUC': 0.51680672268907557,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.96638655462184875,\n",
      " 'error_test': 0.23000000000000001,\n",
      " 'error_train': 0.14000000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 9,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.852\n",
      "iteration 3:\tcost = 0.747\n",
      "iteration 4:\tcost = 0.672\n",
      "iteration 5:\tcost = 0.617\n",
      "iteration 6:\tcost = 0.576\n",
      "iteration 7:\tcost = 0.546\n",
      "iteration 8:\tcost = 0.522\n",
      "iteration 9:\tcost = 0.504\n",
      "iteration 10:\tcost = 0.489\n",
      "iteration 11:\tcost = 0.477\n",
      "iteration 12:\tcost = 0.467\n",
      "iteration 13:\tcost = 0.458\n",
      "iteration 14:\tcost = 0.450\n",
      "iteration 15:\tcost = 0.443\n",
      "iteration 16:\tcost = 0.436\n",
      "iteration 17:\tcost = 0.430\n",
      "iteration 18:\tcost = 0.425\n",
      "iteration 19:\tcost = 0.419\n",
      "iteration 20:\tcost = 0.414\n",
      "{'AUC': 0.51376146788990829,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.97247706422018354,\n",
      " 'error_test': 0.21199999999999999,\n",
      " 'error_train': 0.14399999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 10,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.860\n",
      "iteration 3:\tcost = 0.760\n",
      "iteration 4:\tcost = 0.688\n",
      "iteration 5:\tcost = 0.635\n",
      "iteration 6:\tcost = 0.596\n",
      "iteration 7:\tcost = 0.567\n",
      "iteration 8:\tcost = 0.545\n",
      "iteration 9:\tcost = 0.527\n",
      "iteration 10:\tcost = 0.513\n",
      "iteration 11:\tcost = 0.501\n",
      "iteration 12:\tcost = 0.491\n",
      "iteration 13:\tcost = 0.483\n",
      "iteration 14:\tcost = 0.475\n",
      "iteration 15:\tcost = 0.468\n",
      "iteration 16:\tcost = 0.462\n",
      "iteration 17:\tcost = 0.456\n",
      "iteration 18:\tcost = 0.451\n",
      "iteration 19:\tcost = 0.446\n",
      "iteration 20:\tcost = 0.442\n",
      "{'AUC': 0.50458715596330272,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.99082568807339455,\n",
      " 'error_test': 0.216,\n",
      " 'error_train': 0.14999999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 1,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.846\n",
      "iteration 3:\tcost = 0.737\n",
      "iteration 4:\tcost = 0.658\n",
      "iteration 5:\tcost = 0.601\n",
      "iteration 6:\tcost = 0.559\n",
      "iteration 7:\tcost = 0.527\n",
      "iteration 8:\tcost = 0.503\n",
      "iteration 9:\tcost = 0.484\n",
      "iteration 10:\tcost = 0.469\n",
      "iteration 11:\tcost = 0.456\n",
      "iteration 12:\tcost = 0.445\n",
      "iteration 13:\tcost = 0.436\n",
      "iteration 14:\tcost = 0.428\n",
      "iteration 15:\tcost = 0.420\n",
      "iteration 16:\tcost = 0.413\n",
      "iteration 17:\tcost = 0.407\n",
      "iteration 18:\tcost = 0.401\n",
      "iteration 19:\tcost = 0.396\n",
      "iteration 20:\tcost = 0.390\n",
      "{'AUC': 0.5280373831775701,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.94392523364485981,\n",
      " 'error_test': 0.20200000000000001,\n",
      " 'error_train': 0.14399999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 2,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.858\n",
      "iteration 3:\tcost = 0.755\n",
      "iteration 4:\tcost = 0.681\n",
      "iteration 5:\tcost = 0.626\n",
      "iteration 6:\tcost = 0.584\n",
      "iteration 7:\tcost = 0.553\n",
      "iteration 8:\tcost = 0.528\n",
      "iteration 9:\tcost = 0.508\n",
      "iteration 10:\tcost = 0.492\n",
      "iteration 11:\tcost = 0.478\n",
      "iteration 12:\tcost = 0.466\n",
      "iteration 13:\tcost = 0.456\n",
      "iteration 14:\tcost = 0.447\n",
      "iteration 15:\tcost = 0.438\n",
      "iteration 16:\tcost = 0.431\n",
      "iteration 17:\tcost = 0.424\n",
      "iteration 18:\tcost = 0.417\n",
      "iteration 19:\tcost = 0.411\n",
      "iteration 20:\tcost = 0.406\n",
      "{'AUC': 0.53499999999999992,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.93000000000000005,\n",
      " 'error_test': 0.186,\n",
      " 'error_train': 0.14599999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 3,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.844\n",
      "iteration 3:\tcost = 0.733\n",
      "iteration 4:\tcost = 0.653\n",
      "iteration 5:\tcost = 0.595\n",
      "iteration 6:\tcost = 0.552\n",
      "iteration 7:\tcost = 0.519\n",
      "iteration 8:\tcost = 0.494\n",
      "iteration 9:\tcost = 0.475\n",
      "iteration 10:\tcost = 0.459\n",
      "iteration 11:\tcost = 0.446\n",
      "iteration 12:\tcost = 0.434\n",
      "iteration 13:\tcost = 0.425\n",
      "iteration 14:\tcost = 0.416\n",
      "iteration 15:\tcost = 0.408\n",
      "iteration 16:\tcost = 0.401\n",
      "iteration 17:\tcost = 0.395\n",
      "iteration 18:\tcost = 0.389\n",
      "iteration 19:\tcost = 0.383\n",
      "iteration 20:\tcost = 0.378\n",
      "{'AUC': 0.50862068965517238,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.98275862068965514,\n",
      " 'error_test': 0.22800000000000001,\n",
      " 'error_train': 0.13400000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 4,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.850\n",
      "iteration 3:\tcost = 0.744\n",
      "iteration 4:\tcost = 0.667\n",
      "iteration 5:\tcost = 0.611\n",
      "iteration 6:\tcost = 0.570\n",
      "iteration 7:\tcost = 0.539\n",
      "iteration 8:\tcost = 0.515\n",
      "iteration 9:\tcost = 0.497\n",
      "iteration 10:\tcost = 0.482\n",
      "iteration 11:\tcost = 0.469\n",
      "iteration 12:\tcost = 0.459\n",
      "iteration 13:\tcost = 0.450\n",
      "iteration 14:\tcost = 0.442\n",
      "iteration 15:\tcost = 0.435\n",
      "iteration 16:\tcost = 0.428\n",
      "iteration 17:\tcost = 0.422\n",
      "iteration 18:\tcost = 0.416\n",
      "iteration 19:\tcost = 0.411\n",
      "iteration 20:\tcost = 0.406\n",
      "{'AUC': 0.52380952380952384,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.95238095238095233,\n",
      " 'error_test': 0.20000000000000001,\n",
      " 'error_train': 0.14799999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 5,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.843\n",
      "iteration 3:\tcost = 0.732\n",
      "iteration 4:\tcost = 0.652\n",
      "iteration 5:\tcost = 0.594\n",
      "iteration 6:\tcost = 0.552\n",
      "iteration 7:\tcost = 0.520\n",
      "iteration 8:\tcost = 0.496\n",
      "iteration 9:\tcost = 0.477\n",
      "iteration 10:\tcost = 0.462\n",
      "iteration 11:\tcost = 0.449\n",
      "iteration 12:\tcost = 0.439\n",
      "iteration 13:\tcost = 0.430\n",
      "iteration 14:\tcost = 0.422\n",
      "iteration 15:\tcost = 0.415\n",
      "iteration 16:\tcost = 0.408\n",
      "iteration 17:\tcost = 0.402\n",
      "iteration 18:\tcost = 0.397\n",
      "iteration 19:\tcost = 0.391\n",
      "iteration 20:\tcost = 0.386\n",
      "{'AUC': 0.50909090909090904,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.98181818181818181,\n",
      " 'error_test': 0.216,\n",
      " 'error_train': 0.13800000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 6,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.854\n",
      "iteration 3:\tcost = 0.751\n",
      "iteration 4:\tcost = 0.677\n",
      "iteration 5:\tcost = 0.623\n",
      "iteration 6:\tcost = 0.584\n",
      "iteration 7:\tcost = 0.555\n",
      "iteration 8:\tcost = 0.534\n",
      "iteration 9:\tcost = 0.517\n",
      "iteration 10:\tcost = 0.504\n",
      "iteration 11:\tcost = 0.493\n",
      "iteration 12:\tcost = 0.484\n",
      "iteration 13:\tcost = 0.477\n",
      "iteration 14:\tcost = 0.470\n",
      "iteration 15:\tcost = 0.464\n",
      "iteration 16:\tcost = 0.459\n",
      "iteration 17:\tcost = 0.454\n",
      "iteration 18:\tcost = 0.450\n",
      "iteration 19:\tcost = 0.446\n",
      "iteration 20:\tcost = 0.442\n",
      "{'AUC': 0.50970873786407767,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.98058252427184467,\n",
      " 'error_test': 0.20200000000000001,\n",
      " 'error_train': 0.152}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 7,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.844\n",
      "iteration 3:\tcost = 0.733\n",
      "iteration 4:\tcost = 0.654\n",
      "iteration 5:\tcost = 0.598\n",
      "iteration 6:\tcost = 0.557\n",
      "iteration 7:\tcost = 0.527\n",
      "iteration 8:\tcost = 0.504\n",
      "iteration 9:\tcost = 0.487\n",
      "iteration 10:\tcost = 0.474\n",
      "iteration 11:\tcost = 0.463\n",
      "iteration 12:\tcost = 0.454\n",
      "iteration 13:\tcost = 0.446\n",
      "iteration 14:\tcost = 0.440\n",
      "iteration 15:\tcost = 0.434\n",
      "iteration 16:\tcost = 0.429\n",
      "iteration 17:\tcost = 0.424\n",
      "iteration 18:\tcost = 0.420\n",
      "iteration 19:\tcost = 0.416\n",
      "iteration 20:\tcost = 0.412\n",
      "{'AUC': 0.51339285714285721,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.9732142857142857,\n",
      " 'error_test': 0.218,\n",
      " 'error_train': 0.13800000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 8,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.844\n",
      "iteration 3:\tcost = 0.733\n",
      "iteration 4:\tcost = 0.654\n",
      "iteration 5:\tcost = 0.596\n",
      "iteration 6:\tcost = 0.553\n",
      "iteration 7:\tcost = 0.522\n",
      "iteration 8:\tcost = 0.497\n",
      "iteration 9:\tcost = 0.478\n",
      "iteration 10:\tcost = 0.463\n",
      "iteration 11:\tcost = 0.451\n",
      "iteration 12:\tcost = 0.440\n",
      "iteration 13:\tcost = 0.431\n",
      "iteration 14:\tcost = 0.423\n",
      "iteration 15:\tcost = 0.416\n",
      "iteration 16:\tcost = 0.410\n",
      "iteration 17:\tcost = 0.404\n",
      "iteration 18:\tcost = 0.398\n",
      "iteration 19:\tcost = 0.393\n",
      "iteration 20:\tcost = 0.388\n",
      "{'AUC': 0.5185185185185186,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.96296296296296291,\n",
      " 'error_test': 0.20799999999999999,\n",
      " 'error_train': 0.14000000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 9,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.816\n",
      "iteration 3:\tcost = 0.687\n",
      "iteration 4:\tcost = 0.595\n",
      "iteration 5:\tcost = 0.529\n",
      "iteration 6:\tcost = 0.482\n",
      "iteration 7:\tcost = 0.448\n",
      "iteration 8:\tcost = 0.422\n",
      "iteration 9:\tcost = 0.403\n",
      "iteration 10:\tcost = 0.388\n",
      "iteration 11:\tcost = 0.377\n",
      "iteration 12:\tcost = 0.367\n",
      "iteration 13:\tcost = 0.359\n",
      "iteration 14:\tcost = 0.353\n",
      "iteration 15:\tcost = 0.347\n",
      "iteration 16:\tcost = 0.342\n",
      "iteration 17:\tcost = 0.337\n",
      "iteration 18:\tcost = 0.333\n",
      "iteration 19:\tcost = 0.329\n",
      "iteration 20:\tcost = 0.325\n",
      "{'AUC': 0.50787401574803148,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.98425196850393704,\n",
      " 'error_test': 0.25,\n",
      " 'error_train': 0.108}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 10,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.857\n",
      "iteration 3:\tcost = 0.755\n",
      "iteration 4:\tcost = 0.682\n",
      "iteration 5:\tcost = 0.629\n",
      "iteration 6:\tcost = 0.590\n",
      "iteration 7:\tcost = 0.561\n",
      "iteration 8:\tcost = 0.539\n",
      "iteration 9:\tcost = 0.521\n",
      "iteration 10:\tcost = 0.507\n",
      "iteration 11:\tcost = 0.496\n",
      "iteration 12:\tcost = 0.487\n",
      "iteration 13:\tcost = 0.479\n",
      "iteration 14:\tcost = 0.471\n",
      "iteration 15:\tcost = 0.465\n",
      "iteration 16:\tcost = 0.459\n",
      "iteration 17:\tcost = 0.454\n",
      "iteration 18:\tcost = 0.449\n",
      "iteration 19:\tcost = 0.444\n",
      "iteration 20:\tcost = 0.440\n",
      "{'AUC': 0.51030927835051543,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.97938144329896903,\n",
      " 'error_test': 0.19,\n",
      " 'error_train': 0.152}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 1,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.834\n",
      "iteration 3:\tcost = 0.715\n",
      "iteration 4:\tcost = 0.631\n",
      "iteration 5:\tcost = 0.569\n",
      "iteration 6:\tcost = 0.524\n",
      "iteration 7:\tcost = 0.490\n",
      "iteration 8:\tcost = 0.464\n",
      "iteration 9:\tcost = 0.444\n",
      "iteration 10:\tcost = 0.428\n",
      "iteration 11:\tcost = 0.415\n",
      "iteration 12:\tcost = 0.404\n",
      "iteration 13:\tcost = 0.394\n",
      "iteration 14:\tcost = 0.386\n",
      "iteration 15:\tcost = 0.378\n",
      "iteration 16:\tcost = 0.372\n",
      "iteration 17:\tcost = 0.366\n",
      "iteration 18:\tcost = 0.360\n",
      "iteration 19:\tcost = 0.355\n",
      "iteration 20:\tcost = 0.350\n",
      "{'AUC': 0.51801801801801806,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.963963963963964,\n",
      " 'error_test': 0.214,\n",
      " 'error_train': 0.122}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 2,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.821\n",
      "iteration 3:\tcost = 0.695\n",
      "iteration 4:\tcost = 0.606\n",
      "iteration 5:\tcost = 0.542\n",
      "iteration 6:\tcost = 0.496\n",
      "iteration 7:\tcost = 0.462\n",
      "iteration 8:\tcost = 0.438\n",
      "iteration 9:\tcost = 0.419\n",
      "iteration 10:\tcost = 0.405\n",
      "iteration 11:\tcost = 0.393\n",
      "iteration 12:\tcost = 0.384\n",
      "iteration 13:\tcost = 0.377\n",
      "iteration 14:\tcost = 0.371\n",
      "iteration 15:\tcost = 0.365\n",
      "iteration 16:\tcost = 0.360\n",
      "iteration 17:\tcost = 0.356\n",
      "iteration 18:\tcost = 0.352\n",
      "iteration 19:\tcost = 0.348\n",
      "iteration 20:\tcost = 0.345\n",
      "{'AUC': 0.504,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.99199999999999999,\n",
      " 'error_test': 0.248,\n",
      " 'error_train': 0.112}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 3,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.843\n",
      "iteration 3:\tcost = 0.731\n",
      "iteration 4:\tcost = 0.651\n",
      "iteration 5:\tcost = 0.593\n",
      "iteration 6:\tcost = 0.551\n",
      "iteration 7:\tcost = 0.519\n",
      "iteration 8:\tcost = 0.496\n",
      "iteration 9:\tcost = 0.478\n",
      "iteration 10:\tcost = 0.463\n",
      "iteration 11:\tcost = 0.452\n",
      "iteration 12:\tcost = 0.442\n",
      "iteration 13:\tcost = 0.434\n",
      "iteration 14:\tcost = 0.426\n",
      "iteration 15:\tcost = 0.420\n",
      "iteration 16:\tcost = 0.414\n",
      "iteration 17:\tcost = 0.409\n",
      "iteration 18:\tcost = 0.404\n",
      "iteration 19:\tcost = 0.399\n",
      "iteration 20:\tcost = 0.394\n",
      "{'AUC': 0.52500000000000002,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.94999999999999996,\n",
      " 'error_test': 0.19,\n",
      " 'error_train': 0.13800000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 4,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.837\n",
      "iteration 3:\tcost = 0.721\n",
      "iteration 4:\tcost = 0.638\n",
      "iteration 5:\tcost = 0.579\n",
      "iteration 6:\tcost = 0.535\n",
      "iteration 7:\tcost = 0.503\n",
      "iteration 8:\tcost = 0.479\n",
      "iteration 9:\tcost = 0.460\n",
      "iteration 10:\tcost = 0.445\n",
      "iteration 11:\tcost = 0.434\n",
      "iteration 12:\tcost = 0.424\n",
      "iteration 13:\tcost = 0.415\n",
      "iteration 14:\tcost = 0.408\n",
      "iteration 15:\tcost = 0.402\n",
      "iteration 16:\tcost = 0.396\n",
      "iteration 17:\tcost = 0.390\n",
      "iteration 18:\tcost = 0.386\n",
      "iteration 19:\tcost = 0.381\n",
      "iteration 20:\tcost = 0.376\n",
      "{'AUC': 0.50869565217391299,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.9826086956521739,\n",
      " 'error_test': 0.22600000000000001,\n",
      " 'error_train': 0.13400000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 5,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.845\n",
      "iteration 3:\tcost = 0.735\n",
      "iteration 4:\tcost = 0.656\n",
      "iteration 5:\tcost = 0.599\n",
      "iteration 6:\tcost = 0.557\n",
      "iteration 7:\tcost = 0.527\n",
      "iteration 8:\tcost = 0.504\n",
      "iteration 9:\tcost = 0.486\n",
      "iteration 10:\tcost = 0.472\n",
      "iteration 11:\tcost = 0.461\n",
      "iteration 12:\tcost = 0.452\n",
      "iteration 13:\tcost = 0.444\n",
      "iteration 14:\tcost = 0.437\n",
      "iteration 15:\tcost = 0.431\n",
      "iteration 16:\tcost = 0.425\n",
      "iteration 17:\tcost = 0.420\n",
      "iteration 18:\tcost = 0.415\n",
      "iteration 19:\tcost = 0.411\n",
      "iteration 20:\tcost = 0.407\n",
      "{'AUC': 0.52314814814814814,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.95370370370370372,\n",
      " 'error_test': 0.20599999999999999,\n",
      " 'error_train': 0.14000000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 6,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.837\n",
      "iteration 3:\tcost = 0.721\n",
      "iteration 4:\tcost = 0.638\n",
      "iteration 5:\tcost = 0.578\n",
      "iteration 6:\tcost = 0.534\n",
      "iteration 7:\tcost = 0.501\n",
      "iteration 8:\tcost = 0.476\n",
      "iteration 9:\tcost = 0.456\n",
      "iteration 10:\tcost = 0.441\n",
      "iteration 11:\tcost = 0.428\n",
      "iteration 12:\tcost = 0.418\n",
      "iteration 13:\tcost = 0.409\n",
      "iteration 14:\tcost = 0.401\n",
      "iteration 15:\tcost = 0.394\n",
      "iteration 16:\tcost = 0.387\n",
      "iteration 17:\tcost = 0.382\n",
      "iteration 18:\tcost = 0.376\n",
      "iteration 19:\tcost = 0.371\n",
      "iteration 20:\tcost = 0.367\n",
      "{'AUC': 0.51834862385321101,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.96330275229357798,\n",
      " 'error_test': 0.20999999999999999,\n",
      " 'error_train': 0.128}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 7,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.838\n",
      "iteration 3:\tcost = 0.723\n",
      "iteration 4:\tcost = 0.641\n",
      "iteration 5:\tcost = 0.581\n",
      "iteration 6:\tcost = 0.538\n",
      "iteration 7:\tcost = 0.506\n",
      "iteration 8:\tcost = 0.481\n",
      "iteration 9:\tcost = 0.463\n",
      "iteration 10:\tcost = 0.448\n",
      "iteration 11:\tcost = 0.436\n",
      "iteration 12:\tcost = 0.427\n",
      "iteration 13:\tcost = 0.419\n",
      "iteration 14:\tcost = 0.411\n",
      "iteration 15:\tcost = 0.405\n",
      "iteration 16:\tcost = 0.400\n",
      "iteration 17:\tcost = 0.394\n",
      "iteration 18:\tcost = 0.390\n",
      "iteration 19:\tcost = 0.385\n",
      "iteration 20:\tcost = 0.381\n",
      "{'AUC': 0.51428571428571423,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.97142857142857142,\n",
      " 'error_test': 0.20399999999999999,\n",
      " 'error_train': 0.13200000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 8,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.856\n",
      "iteration 3:\tcost = 0.754\n",
      "iteration 4:\tcost = 0.680\n",
      "iteration 5:\tcost = 0.627\n",
      "iteration 6:\tcost = 0.588\n",
      "iteration 7:\tcost = 0.558\n",
      "iteration 8:\tcost = 0.536\n",
      "iteration 9:\tcost = 0.518\n",
      "iteration 10:\tcost = 0.504\n",
      "iteration 11:\tcost = 0.493\n",
      "iteration 12:\tcost = 0.483\n",
      "iteration 13:\tcost = 0.475\n",
      "iteration 14:\tcost = 0.468\n",
      "iteration 15:\tcost = 0.461\n",
      "iteration 16:\tcost = 0.455\n",
      "iteration 17:\tcost = 0.450\n",
      "iteration 18:\tcost = 0.445\n",
      "iteration 19:\tcost = 0.440\n",
      "iteration 20:\tcost = 0.436\n",
      "{'AUC': 0.52105263157894743,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.95789473684210524,\n",
      " 'error_test': 0.182,\n",
      " 'error_train': 0.14999999999999999}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 9,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.825\n",
      "iteration 3:\tcost = 0.701\n",
      "iteration 4:\tcost = 0.613\n",
      "iteration 5:\tcost = 0.550\n",
      "iteration 6:\tcost = 0.505\n",
      "iteration 7:\tcost = 0.472\n",
      "iteration 8:\tcost = 0.448\n",
      "iteration 9:\tcost = 0.429\n",
      "iteration 10:\tcost = 0.415\n",
      "iteration 11:\tcost = 0.404\n",
      "iteration 12:\tcost = 0.396\n",
      "iteration 13:\tcost = 0.388\n",
      "iteration 14:\tcost = 0.382\n",
      "iteration 15:\tcost = 0.377\n",
      "iteration 16:\tcost = 0.372\n",
      "iteration 17:\tcost = 0.368\n",
      "iteration 18:\tcost = 0.364\n",
      "iteration 19:\tcost = 0.360\n",
      "iteration 20:\tcost = 0.357\n",
      "{'AUC': 0.5083333333333333,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.98333333333333328,\n",
      " 'error_test': 0.23599999999999999,\n",
      " 'error_train': 0.11600000000000001}\n",
      "\n",
      "{'add_bias': True,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.adaline' from '../classifiers/adaline.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 10,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "iteration 1:\tcost = 1.000\n",
      "iteration 2:\tcost = 0.838\n",
      "iteration 3:\tcost = 0.723\n",
      "iteration 4:\tcost = 0.642\n",
      "iteration 5:\tcost = 0.583\n",
      "iteration 6:\tcost = 0.542\n",
      "iteration 7:\tcost = 0.511\n",
      "iteration 8:\tcost = 0.488\n",
      "iteration 9:\tcost = 0.471\n",
      "iteration 10:\tcost = 0.458\n",
      "iteration 11:\tcost = 0.448\n",
      "iteration 12:\tcost = 0.439\n",
      "iteration 13:\tcost = 0.433\n",
      "iteration 14:\tcost = 0.427\n",
      "iteration 15:\tcost = 0.421\n",
      "iteration 16:\tcost = 0.417\n",
      "iteration 17:\tcost = 0.413\n",
      "iteration 18:\tcost = 0.409\n",
      "iteration 19:\tcost = 0.405\n",
      "iteration 20:\tcost = 0.402\n",
      "{'AUC': 0.50454545454545452,\n",
      " 'FNR': 0.0,\n",
      " 'FPR': 0.99090909090909096,\n",
      " 'error_test': 0.218,\n",
      " 'error_train': 0.13200000000000001}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 1,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.70095836008656154,\n",
      " 'FNR': 0.027989821882951654,\n",
      " 'FPR': 0.57009345794392519,\n",
      " 'error_test': 0.14399999999999999,\n",
      " 'error_train': 0.104}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 2,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.7184417201417489,\n",
      " 'FNR': 0.025380710659898477,\n",
      " 'FPR': 0.53773584905660377,\n",
      " 'error_test': 0.13400000000000001,\n",
      " 'error_train': 0.114}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 3,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.76169079518524607,\n",
      " 'FNR': 0.017902813299232736,\n",
      " 'FPR': 0.45871559633027525,\n",
      " 'error_test': 0.114,\n",
      " 'error_train': 0.13400000000000001}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 4,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73491869823318234,\n",
      " 'FNR': 0.02557544757033248,\n",
      " 'FPR': 0.50458715596330272,\n",
      " 'error_test': 0.13,\n",
      " 'error_train': 0.112}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 5,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73461538461538467,\n",
      " 'FNR': 0.030769230769230771,\n",
      " 'FPR': 0.5,\n",
      " 'error_test': 0.13400000000000001,\n",
      " 'error_train': 0.106}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 6,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73074363297541933,\n",
      " 'FNR': 0.013089005235602094,\n",
      " 'FPR': 0.52542372881355937,\n",
      " 'error_test': 0.13400000000000001,\n",
      " 'error_train': 0.124}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 7,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.74740067991761583,\n",
      " 'FNR': 0.020050125313283207,\n",
      " 'FPR': 0.48514851485148514,\n",
      " 'error_test': 0.114,\n",
      " 'error_train': 0.13}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 8,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.70586340206185572,\n",
      " 'FNR': 0.025773195876288658,\n",
      " 'FPR': 0.5625,\n",
      " 'error_test': 0.14599999999999999,\n",
      " 'error_train': 0.108}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 9,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.74894514767932485,\n",
      " 'FNR': 0.035443037974683546,\n",
      " 'FPR': 0.46666666666666667,\n",
      " 'error_test': 0.126,\n",
      " 'error_train': 0.124}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 10,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73660040819948525,\n",
      " 'FNR': 0.01832460732984293,\n",
      " 'FPR': 0.50847457627118642,\n",
      " 'error_test': 0.13400000000000001,\n",
      " 'error_train': 0.112}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 1,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.7384615384615385,\n",
      " 'FNR': 0.023076923076923078,\n",
      " 'FPR': 0.5,\n",
      " 'error_test': 0.128,\n",
      " 'error_train': 0.10199999999999999}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 2,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.76895893961996531,\n",
      " 'FNR': 0.025188916876574308,\n",
      " 'FPR': 0.43689320388349512,\n",
      " 'error_test': 0.11,\n",
      " 'error_train': 0.126}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 3,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73769914826720484,\n",
      " 'FNR': 0.038363171355498722,\n",
      " 'FPR': 0.48623853211009177,\n",
      " 'error_test': 0.13600000000000001,\n",
      " 'error_train': 0.106}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 4,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.70654997176736301,\n",
      " 'FNR': 0.012987012987012988,\n",
      " 'FPR': 0.57391304347826089,\n",
      " 'error_test': 0.14199999999999999,\n",
      " 'error_train': 0.096000000000000002}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 5,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.74157768777614141,\n",
      " 'FNR': 0.025773195876288658,\n",
      " 'FPR': 0.49107142857142855,\n",
      " 'error_test': 0.13,\n",
      " 'error_train': 0.10199999999999999}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 6,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.76000000000000001,\n",
      " 'FNR': 0.029999999999999999,\n",
      " 'FPR': 0.45000000000000001,\n",
      " 'error_test': 0.114,\n",
      " 'error_train': 0.124}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 7,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.72650634483982612,\n",
      " 'FNR': 0.013089005235602094,\n",
      " 'FPR': 0.53389830508474578,\n",
      " 'error_test': 0.13600000000000001,\n",
      " 'error_train': 0.104}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 8,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.75811567880074704,\n",
      " 'FNR': 0.019851116625310174,\n",
      " 'FPR': 0.46391752577319589,\n",
      " 'error_test': 0.106,\n",
      " 'error_train': 0.122}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 9,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.70388438880706916,\n",
      " 'FNR': 0.038659793814432991,\n",
      " 'FPR': 0.5535714285714286,\n",
      " 'error_test': 0.154,\n",
      " 'error_train': 0.091999999999999998}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 10,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.71231546231546228,\n",
      " 'FNR': 0.017676767676767676,\n",
      " 'FPR': 0.55769230769230771,\n",
      " 'error_test': 0.13,\n",
      " 'error_train': 0.106}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 1,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.75147679324894512,\n",
      " 'FNR': 0.030379746835443037,\n",
      " 'FPR': 0.46666666666666667,\n",
      " 'error_test': 0.122,\n",
      " 'error_train': 0.087999999999999995}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 2,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.69797388884925449,\n",
      " 'FNR': 0.015267175572519083,\n",
      " 'FPR': 0.58878504672897192,\n",
      " 'error_test': 0.13800000000000001,\n",
      " 'error_train': 0.089999999999999997}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 3,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73019891500904155,\n",
      " 'FNR': 0.025316455696202531,\n",
      " 'FPR': 0.51428571428571423,\n",
      " 'error_test': 0.128,\n",
      " 'error_train': 0.10000000000000001}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 4,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.75786713286713281,\n",
      " 'FNR': 0.022727272727272728,\n",
      " 'FPR': 0.46153846153846156,\n",
      " 'error_test': 0.114,\n",
      " 'error_train': 0.106}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 5,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.70148337112622827,\n",
      " 'FNR': 0.022959183673469389,\n",
      " 'FPR': 0.57407407407407407,\n",
      " 'error_test': 0.14199999999999999,\n",
      " 'error_train': 0.087999999999999995}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 6,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73076923076923084,\n",
      " 'FNR': 0.038461538461538464,\n",
      " 'FPR': 0.5,\n",
      " 'error_test': 0.14000000000000001,\n",
      " 'error_train': 0.10000000000000001}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 7,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.69696969696969702,\n",
      " 'FNR': 0.033333333333333333,\n",
      " 'FPR': 0.57272727272727275,\n",
      " 'error_test': 0.152,\n",
      " 'error_train': 0.085999999999999993}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 8,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.75685751104565546,\n",
      " 'FNR': 0.030927835051546393,\n",
      " 'FPR': 0.45535714285714285,\n",
      " 'error_test': 0.126,\n",
      " 'error_train': 0.098000000000000004}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 9,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.75258945899387242,\n",
      " 'FNR': 0.023746701846965697,\n",
      " 'FPR': 0.47107438016528924,\n",
      " 'error_test': 0.13200000000000001,\n",
      " 'error_train': 0.087999999999999995}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 10,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.79411287894578386,\n",
      " 'FNR': 0.051413881748071981,\n",
      " 'FPR': 0.36036036036036034,\n",
      " 'error_test': 0.12,\n",
      " 'error_train': 0.10000000000000001}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 1,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73942699363022502,\n",
      " 'FNR': 0.026763990267639901,\n",
      " 'FPR': 0.4943820224719101,\n",
      " 'error_test': 0.11,\n",
      " 'error_train': 0.112}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 2,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.71935159467483956,\n",
      " 'FNR': 0.032994923857868022,\n",
      " 'FPR': 0.52830188679245282,\n",
      " 'error_test': 0.13800000000000001,\n",
      " 'error_train': 0.085999999999999993}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 3,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.70980106070080362,\n",
      " 'FNR': 0.030848329048843187,\n",
      " 'FPR': 0.5495495495495496,\n",
      " 'error_test': 0.14599999999999999,\n",
      " 'error_train': 0.089999999999999997}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 4,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.7265350877192982,\n",
      " 'FNR': 0.05526315789473684,\n",
      " 'FPR': 0.49166666666666664,\n",
      " 'error_test': 0.16,\n",
      " 'error_train': 0.071999999999999995}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 5,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.75578874765986803,\n",
      " 'FNR': 0.027638190954773871,\n",
      " 'FPR': 0.46078431372549017,\n",
      " 'error_test': 0.11600000000000001,\n",
      " 'error_train': 0.096000000000000002}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 6,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.74091880341880345,\n",
      " 'FNR': 0.027777777777777776,\n",
      " 'FPR': 0.49038461538461536,\n",
      " 'error_test': 0.124,\n",
      " 'error_train': 0.098000000000000004}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 7,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73132362800498041,\n",
      " 'FNR': 0.027918781725888325,\n",
      " 'FPR': 0.50943396226415094,\n",
      " 'error_test': 0.13,\n",
      " 'error_train': 0.10000000000000001}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 8,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.71986016628873772,\n",
      " 'FNR': 0.051020408163265307,\n",
      " 'FPR': 0.5092592592592593,\n",
      " 'error_test': 0.14999999999999999,\n",
      " 'error_train': 0.082000000000000003}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 9,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.7341678594054295,\n",
      " 'FNR': 0.017902813299232736,\n",
      " 'FPR': 0.51376146788990829,\n",
      " 'error_test': 0.126,\n",
      " 'error_train': 0.085999999999999993}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 10,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.75625920471281294,\n",
      " 'FNR': 0.023195876288659795,\n",
      " 'FPR': 0.4642857142857143,\n",
      " 'error_test': 0.122,\n",
      " 'error_train': 0.082000000000000003}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 1,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.71697845804988658,\n",
      " 'FNR': 0.03826530612244898,\n",
      " 'FPR': 0.52777777777777779,\n",
      " 'error_test': 0.14399999999999999,\n",
      " 'error_train': 0.084000000000000005}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 2,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.75998467579733742,\n",
      " 'FNR': 0.017766497461928935,\n",
      " 'FPR': 0.46226415094339623,\n",
      " 'error_test': 0.112,\n",
      " 'error_train': 0.104}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 3,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.75053418803418803,\n",
      " 'FNR': 0.027777777777777776,\n",
      " 'FPR': 0.47115384615384615,\n",
      " 'error_test': 0.12,\n",
      " 'error_train': 0.091999999999999998}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 4,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.7646667142279614,\n",
      " 'FNR': 0.012722646310432569,\n",
      " 'FPR': 0.45794392523364486,\n",
      " 'error_test': 0.108,\n",
      " 'error_train': 0.094}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 5,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73133822699040096,\n",
      " 'FNR': 0.015584415584415584,\n",
      " 'FPR': 0.52173913043478259,\n",
      " 'error_test': 0.13200000000000001,\n",
      " 'error_train': 0.084000000000000005}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 6,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.71447368421052637,\n",
      " 'FNR': 0.021052631578947368,\n",
      " 'FPR': 0.55000000000000004,\n",
      " 'error_test': 0.14799999999999999,\n",
      " 'error_train': 0.066000000000000003}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 7,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.75957765570378821,\n",
      " 'FNR': 0.022900763358778626,\n",
      " 'FPR': 0.45794392523364486,\n",
      " 'error_test': 0.11600000000000001,\n",
      " 'error_train': 0.091999999999999998}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 8,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.75868102899736378,\n",
      " 'FNR': 0.044041450777202069,\n",
      " 'FPR': 0.43859649122807015,\n",
      " 'error_test': 0.13400000000000001,\n",
      " 'error_train': 0.071999999999999995}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 9,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73982658389989164,\n",
      " 'FNR': 0.03015075376884422,\n",
      " 'FPR': 0.49019607843137253,\n",
      " 'error_test': 0.124,\n",
      " 'error_train': 0.070000000000000007}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 10,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.72331457421431722,\n",
      " 'FNR': 0.030848329048843187,\n",
      " 'FPR': 0.52252252252252251,\n",
      " 'error_test': 0.14000000000000001,\n",
      " 'error_train': 0.073999999999999996}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 1,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73635392385392384,\n",
      " 'FNR': 0.017676767676767676,\n",
      " 'FPR': 0.50961538461538458,\n",
      " 'error_test': 0.12,\n",
      " 'error_train': 0.073999999999999996}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 2,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.72270784770784768,\n",
      " 'FNR': 0.035353535353535352,\n",
      " 'FPR': 0.51923076923076927,\n",
      " 'error_test': 0.13600000000000001,\n",
      " 'error_train': 0.082000000000000003}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 3,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.74153546540275461,\n",
      " 'FNR': 0.030690537084398978,\n",
      " 'FPR': 0.48623853211009177,\n",
      " 'error_test': 0.13,\n",
      " 'error_train': 0.087999999999999995}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 4,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.74063721479865474,\n",
      " 'FNR': 0.036269430051813469,\n",
      " 'FPR': 0.48245614035087719,\n",
      " 'error_test': 0.13800000000000001,\n",
      " 'error_train': 0.066000000000000003}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 5,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.71476336845069111,\n",
      " 'FNR': 0.038363171355498722,\n",
      " 'FPR': 0.5321100917431193,\n",
      " 'error_test': 0.14599999999999999,\n",
      " 'error_train': 0.075999999999999998}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 6,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.75159846547314579,\n",
      " 'FNR': 0.029411764705882353,\n",
      " 'FPR': 0.46739130434782611,\n",
      " 'error_test': 0.11,\n",
      " 'error_train': 0.075999999999999998}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 7,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.72495592600031245,\n",
      " 'FNR': 0.028720626631853787,\n",
      " 'FPR': 0.5213675213675214,\n",
      " 'error_test': 0.14399999999999999,\n",
      " 'error_train': 0.066000000000000003}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 8,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.7150886963696369,\n",
      " 'FNR': 0.059405940594059403,\n",
      " 'FPR': 0.51041666666666663,\n",
      " 'error_test': 0.14599999999999999,\n",
      " 'error_train': 0.080000000000000002}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 9,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.75363949813236286,\n",
      " 'FNR': 0.030456852791878174,\n",
      " 'FPR': 0.46226415094339623,\n",
      " 'error_test': 0.122,\n",
      " 'error_train': 0.073999999999999996}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.ham' from '../attacks/ham.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 10,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.72365802665054535,\n",
      " 'FNR': 0.027431421446384038,\n",
      " 'FPR': 0.5252525252525253,\n",
      " 'error_test': 0.126,\n",
      " 'error_train': 0.075999999999999998}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 1,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73248715849028512,\n",
      " 'FNR': 0.03007518796992481,\n",
      " 'FPR': 0.50495049504950495,\n",
      " 'error_test': 0.126,\n",
      " 'error_train': 0.12}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 2,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.72138076379721627,\n",
      " 'FNR': 0.02570694087403599,\n",
      " 'FPR': 0.53153153153153154,\n",
      " 'error_test': 0.13800000000000001,\n",
      " 'error_train': 0.12}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 3,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.76771115588596306,\n",
      " 'FNR': 0.02313624678663239,\n",
      " 'FPR': 0.44144144144144143,\n",
      " 'error_test': 0.11600000000000001,\n",
      " 'error_train': 0.124}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 4,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.74557109557109558,\n",
      " 'FNR': 0.017948717948717947,\n",
      " 'FPR': 0.49090909090909091,\n",
      " 'error_test': 0.122,\n",
      " 'error_train': 0.12}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 5,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73875000000000002,\n",
      " 'FNR': 0.032500000000000001,\n",
      " 'FPR': 0.48999999999999999,\n",
      " 'error_test': 0.124,\n",
      " 'error_train': 0.128}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 6,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.77572471522674857,\n",
      " 'FNR': 0.027989821882951654,\n",
      " 'FPR': 0.42056074766355139,\n",
      " 'error_test': 0.112,\n",
      " 'error_train': 0.126}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 7,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.74859177647088015,\n",
      " 'FNR': 0.027568922305764409,\n",
      " 'FPR': 0.47524752475247523,\n",
      " 'error_test': 0.11799999999999999,\n",
      " 'error_train': 0.126}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 8,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73302691454574553,\n",
      " 'FNR': 0.020671834625322998,\n",
      " 'FPR': 0.51327433628318586,\n",
      " 'error_test': 0.13200000000000001,\n",
      " 'error_train': 0.11600000000000001}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 9,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73184039633125797,\n",
      " 'FNR': 0.023498694516971279,\n",
      " 'FPR': 0.51282051282051277,\n",
      " 'error_test': 0.13800000000000001,\n",
      " 'error_train': 0.11}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 10,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.71445221445221452,\n",
      " 'FNR': 0.02564102564102564,\n",
      " 'FPR': 0.54545454545454541,\n",
      " 'error_test': 0.14000000000000001,\n",
      " 'error_train': 0.11}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 1,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.7639211562446504,\n",
      " 'FNR': 0.035264483627204031,\n",
      " 'FPR': 0.43689320388349512,\n",
      " 'error_test': 0.11799999999999999,\n",
      " 'error_train': 0.12}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 2,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73499999999999999,\n",
      " 'FNR': 0.029999999999999999,\n",
      " 'FPR': 0.5,\n",
      " 'error_test': 0.124,\n",
      " 'error_train': 0.112}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 3,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.75477855477855482,\n",
      " 'FNR': 0.035897435897435895,\n",
      " 'FPR': 0.45454545454545453,\n",
      " 'error_test': 0.128,\n",
      " 'error_train': 0.112}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 4,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.75354227152038511,\n",
      " 'FNR': 0.027568922305764409,\n",
      " 'FPR': 0.46534653465346537,\n",
      " 'error_test': 0.11600000000000001,\n",
      " 'error_train': 0.11799999999999999}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 5,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.75497054491899851,\n",
      " 'FNR': 0.025773195876288658,\n",
      " 'FPR': 0.4642857142857143,\n",
      " 'error_test': 0.124,\n",
      " 'error_train': 0.112}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 6,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.72868112787654993,\n",
      " 'FNR': 0.037783375314861464,\n",
      " 'FPR': 0.50485436893203883,\n",
      " 'error_test': 0.13400000000000001,\n",
      " 'error_train': 0.112}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 7,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73514851485148514,\n",
      " 'FNR': 0.029702970297029702,\n",
      " 'FPR': 0.5,\n",
      " 'error_test': 0.12,\n",
      " 'error_train': 0.128}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 8,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.74133617994004875,\n",
      " 'FNR': 0.013262599469496022,\n",
      " 'FPR': 0.50406504065040647,\n",
      " 'error_test': 0.13400000000000001,\n",
      " 'error_train': 0.108}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 9,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.72874082439299825,\n",
      " 'FNR': 0.020779220779220779,\n",
      " 'FPR': 0.52173913043478259,\n",
      " 'error_test': 0.13600000000000001,\n",
      " 'error_train': 0.098000000000000004}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.1},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 10,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.74664027982326953,\n",
      " 'FNR': 0.033505154639175257,\n",
      " 'FPR': 0.4732142857142857,\n",
      " 'error_test': 0.13200000000000001,\n",
      " 'error_train': 0.104}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 1,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.7430830039525691,\n",
      " 'FNR': 0.018181818181818181,\n",
      " 'FPR': 0.4956521739130435,\n",
      " 'error_test': 0.128,\n",
      " 'error_train': 0.104}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 2,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73460378028219009,\n",
      " 'FNR': 0.039267015706806283,\n",
      " 'FPR': 0.49152542372881358,\n",
      " 'error_test': 0.14599999999999999,\n",
      " 'error_train': 0.084000000000000005}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 3,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.72434729811778986,\n",
      " 'FNR': 0.018518518518518517,\n",
      " 'FPR': 0.53278688524590168,\n",
      " 'error_test': 0.14399999999999999,\n",
      " 'error_train': 0.085999999999999993}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 4,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.72705639367816088,\n",
      " 'FNR': 0.028645833333333332,\n",
      " 'FPR': 0.51724137931034486,\n",
      " 'error_test': 0.14199999999999999,\n",
      " 'error_train': 0.085999999999999993}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 5,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.7436412814213752,\n",
      " 'FNR': 0.027568922305764409,\n",
      " 'FPR': 0.48514851485148514,\n",
      " 'error_test': 0.12,\n",
      " 'error_train': 0.10000000000000001}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 6,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.72989949748743721,\n",
      " 'FNR': 0.040201005025125629,\n",
      " 'FPR': 0.5,\n",
      " 'error_test': 0.13400000000000001,\n",
      " 'error_train': 0.10000000000000001}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 7,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.72593514070006859,\n",
      " 'FNR': 0.023936170212765957,\n",
      " 'FPR': 0.52419354838709675,\n",
      " 'error_test': 0.14799999999999999,\n",
      " 'error_train': 0.091999999999999998}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 8,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73295182453787944,\n",
      " 'FNR': 0.015228426395939087,\n",
      " 'FPR': 0.51886792452830188,\n",
      " 'error_test': 0.122,\n",
      " 'error_train': 0.085999999999999993}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 9,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.70225722041525029,\n",
      " 'FNR': 0.022670025188916875,\n",
      " 'FPR': 0.57281553398058249,\n",
      " 'error_test': 0.13600000000000001,\n",
      " 'error_train': 0.085999999999999993}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.2},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 10,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.71732456140350886,\n",
      " 'FNR': 0.02368421052631579,\n",
      " 'FPR': 0.54166666666666663,\n",
      " 'error_test': 0.14799999999999999,\n",
      " 'error_train': 0.085999999999999993}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 1,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.72947349042709864,\n",
      " 'FNR': 0.023195876288659795,\n",
      " 'FPR': 0.5178571428571429,\n",
      " 'error_test': 0.13400000000000001,\n",
      " 'error_train': 0.085999999999999993}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 2,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.72503039777661971,\n",
      " 'FNR': 0.035087719298245612,\n",
      " 'FPR': 0.51485148514851486,\n",
      " 'error_test': 0.13200000000000001,\n",
      " 'error_train': 0.085999999999999993}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 3,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.74272793382419777,\n",
      " 'FNR': 0.023316062176165803,\n",
      " 'FPR': 0.49122807017543857,\n",
      " 'error_test': 0.13,\n",
      " 'error_train': 0.091999999999999998}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 4,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73456682439651977,\n",
      " 'FNR': 0.018469656992084433,\n",
      " 'FPR': 0.51239669421487599,\n",
      " 'error_test': 0.13800000000000001,\n",
      " 'error_train': 0.078}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 5,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.69254848255117263,\n",
      " 'FNR': 0.022670025188916875,\n",
      " 'FPR': 0.59223300970873782,\n",
      " 'error_test': 0.14000000000000001,\n",
      " 'error_train': 0.082000000000000003}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 6,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.75871372658005065,\n",
      " 'FNR': 0.041131105398457581,\n",
      " 'FPR': 0.44144144144144143,\n",
      " 'error_test': 0.13,\n",
      " 'error_train': 0.094}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 7,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.74387053815604864,\n",
      " 'FNR': 0.035623409669211195,\n",
      " 'FPR': 0.47663551401869159,\n",
      " 'error_test': 0.13,\n",
      " 'error_train': 0.091999999999999998}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 8,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.72371477510030746,\n",
      " 'FNR': 0.020460358056265986,\n",
      " 'FPR': 0.5321100917431193,\n",
      " 'error_test': 0.13200000000000001,\n",
      " 'error_train': 0.071999999999999995}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 9,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.71682032721136679,\n",
      " 'FNR': 0.022670025188916875,\n",
      " 'FPR': 0.5436893203883495,\n",
      " 'error_test': 0.13,\n",
      " 'error_train': 0.085999999999999993}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.3},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 10,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.74798071127185051,\n",
      " 'FNR': 0.027848101265822784,\n",
      " 'FPR': 0.47619047619047616,\n",
      " 'error_test': 0.122,\n",
      " 'error_train': 0.091999999999999998}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 1,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73261526832955404,\n",
      " 'FNR': 0.025510204081632654,\n",
      " 'FPR': 0.5092592592592593,\n",
      " 'error_test': 0.13,\n",
      " 'error_train': 0.073999999999999996}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 2,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73516628873771728,\n",
      " 'FNR': 0.020408163265306121,\n",
      " 'FPR': 0.5092592592592593,\n",
      " 'error_test': 0.126,\n",
      " 'error_train': 0.078}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 3,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.70460526315789473,\n",
      " 'FNR': 0.015789473684210527,\n",
      " 'FPR': 0.57499999999999996,\n",
      " 'error_test': 0.14999999999999999,\n",
      " 'error_train': 0.070000000000000007}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 4,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.76144801980198018,\n",
      " 'FNR': 0.039603960396039604,\n",
      " 'FPR': 0.4375,\n",
      " 'error_test': 0.11600000000000001,\n",
      " 'error_train': 0.087999999999999995}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 5,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.75088678687555421,\n",
      " 'FNR': 0.027638190954773871,\n",
      " 'FPR': 0.47058823529411764,\n",
      " 'error_test': 0.11799999999999999,\n",
      " 'error_train': 0.075999999999999998}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 6,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73173492488166292,\n",
      " 'FNR': 0.023255813953488372,\n",
      " 'FPR': 0.51327433628318586,\n",
      " 'error_test': 0.13400000000000001,\n",
      " 'error_train': 0.068000000000000005}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 7,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.74169288790698706,\n",
      " 'FNR': 0.020887728459530026,\n",
      " 'FPR': 0.49572649572649574,\n",
      " 'error_test': 0.13200000000000001,\n",
      " 'error_train': 0.071999999999999995}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 8,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.75828359744803442,\n",
      " 'FNR': 0.023255813953488372,\n",
      " 'FPR': 0.46017699115044247,\n",
      " 'error_test': 0.122,\n",
      " 'error_train': 0.071999999999999995}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 9,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.74599890609022379,\n",
      " 'FNR': 0.040712468193384227,\n",
      " 'FPR': 0.46728971962616822,\n",
      " 'error_test': 0.13200000000000001,\n",
      " 'error_train': 0.073999999999999996}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.4},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 10,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.77614878579638547,\n",
      " 'FNR': 0.030226700251889168,\n",
      " 'FPR': 0.41747572815533979,\n",
      " 'error_test': 0.11,\n",
      " 'error_train': 0.096000000000000002}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 1,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73210274973287692,\n",
      " 'FNR': 0.031662269129287601,\n",
      " 'FPR': 0.50413223140495866,\n",
      " 'error_test': 0.14599999999999999,\n",
      " 'error_train': 0.056000000000000001}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 2,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73831001472754054,\n",
      " 'FNR': 0.041237113402061855,\n",
      " 'FPR': 0.48214285714285715,\n",
      " 'error_test': 0.14000000000000001,\n",
      " 'error_train': 0.078}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 3,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.71814345991561179,\n",
      " 'FNR': 0.030379746835443037,\n",
      " 'FPR': 0.53333333333333333,\n",
      " 'error_test': 0.13600000000000001,\n",
      " 'error_train': 0.062}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 4,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.74014996894134355,\n",
      " 'FNR': 0.036649214659685861,\n",
      " 'FPR': 0.48305084745762711,\n",
      " 'error_test': 0.14199999999999999,\n",
      " 'error_train': 0.078}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 5,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.72320081639897071,\n",
      " 'FNR': 0.036649214659685861,\n",
      " 'FPR': 0.51694915254237284,\n",
      " 'error_test': 0.14999999999999999,\n",
      " 'error_train': 0.066000000000000003}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 6,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.68803418803418803,\n",
      " 'FNR': 0.027777777777777776,\n",
      " 'FPR': 0.59615384615384615,\n",
      " 'error_test': 0.14599999999999999,\n",
      " 'error_train': 0.064000000000000001}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 7,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73963730569948183,\n",
      " 'FNR': 0.02072538860103627,\n",
      " 'FPR': 0.5,\n",
      " 'error_test': 0.13,\n",
      " 'error_train': 0.070000000000000007}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 8,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.75497054491899851,\n",
      " 'FNR': 0.025773195876288658,\n",
      " 'FPR': 0.4642857142857143,\n",
      " 'error_test': 0.124,\n",
      " 'error_train': 0.068000000000000005}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 9,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.73214285714285721,\n",
      " 'FNR': 0.035714285714285712,\n",
      " 'FPR': 0.5,\n",
      " 'error_test': 0.13600000000000001,\n",
      " 'error_train': 0.062}\n",
      "\n",
      "{'add_bias': False,\n",
      " 'attack': <module 'attacks.empty' from '../attacks/empty.py'>,\n",
      " 'attack_parameters': {'percentage_samples_poisoned': 0.5},\n",
      " 'classifier': <module 'classifiers.naivebayes' from '../classifiers/naivebayes.py'>,\n",
      " 'dataset': 'trec2007',\n",
      " 'dataset_filename': 'trec2007-1607201347',\n",
      " 'iteration': 10,\n",
      " 'label_type': {'ham_label': -1, 'spam_label': 1},\n",
      " 'testing_parameters': {'ham_label': -1},\n",
      " 'training_parameters': {'ham_label': -1}}\n",
      "{'AUC': 0.7233502538071066,\n",
      " 'FNR': 0.053299492385786802,\n",
      " 'FPR': 0.5,\n",
      " 'error_test': 0.14799999999999999,\n",
      " 'error_train': 0.075999999999999998}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%aimport pipeline\n",
    "\n",
    "DF = pipeline.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>metrics</th>\n",
       "      <th>AUC</th>\n",
       "      <th>FNR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>error_test</th>\n",
       "      <th>error_train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classifier</th>\n",
       "      <th>attack</th>\n",
       "      <th>percentage_samples_poisoned</th>\n",
       "      <th>iteration</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">adaline</th>\n",
       "      <th rowspan=\"30\" valign=\"top\">empty</th>\n",
       "      <th rowspan=\"10\" valign=\"top\">0.0</th>\n",
       "      <th>1</th>\n",
       "      <td>0.569565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.860870</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.580357</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.580189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.839623</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.567308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.616391</td>\n",
       "      <td>0.002513</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.626316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.747368</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.601942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.796117</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.597222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.566372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.867257</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">0.1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.597222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.549107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.901786</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.546296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.907407</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.564815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.870370</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.543269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.913462</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.556075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.887850</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.577670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.844660</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.551887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.896226</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">0.2</th>\n",
       "      <th>1</th>\n",
       "      <td>0.534783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.930435</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.542056</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.915888</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.531532</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.936937</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.516529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.966942</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.574257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.851485</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.569307</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.861386</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.512605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.974790</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">naive bayes</th>\n",
       "      <th rowspan=\"30\" valign=\"top\">ham</th>\n",
       "      <th rowspan=\"10\" valign=\"top\">0.3</th>\n",
       "      <th>1</th>\n",
       "      <td>0.739427</td>\n",
       "      <td>0.026764</td>\n",
       "      <td>0.494382</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.719352</td>\n",
       "      <td>0.032995</td>\n",
       "      <td>0.528302</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.709801</td>\n",
       "      <td>0.030848</td>\n",
       "      <td>0.549550</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.726535</td>\n",
       "      <td>0.055263</td>\n",
       "      <td>0.491667</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.755789</td>\n",
       "      <td>0.027638</td>\n",
       "      <td>0.460784</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.740919</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.490385</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.731324</td>\n",
       "      <td>0.027919</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.719860</td>\n",
       "      <td>0.051020</td>\n",
       "      <td>0.509259</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.734168</td>\n",
       "      <td>0.017903</td>\n",
       "      <td>0.513761</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.756259</td>\n",
       "      <td>0.023196</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">0.4</th>\n",
       "      <th>1</th>\n",
       "      <td>0.716978</td>\n",
       "      <td>0.038265</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.759985</td>\n",
       "      <td>0.017766</td>\n",
       "      <td>0.462264</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.750534</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.471154</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.764667</td>\n",
       "      <td>0.012723</td>\n",
       "      <td>0.457944</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.731338</td>\n",
       "      <td>0.015584</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.714474</td>\n",
       "      <td>0.021053</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.759578</td>\n",
       "      <td>0.022901</td>\n",
       "      <td>0.457944</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.758681</td>\n",
       "      <td>0.044041</td>\n",
       "      <td>0.438596</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.739827</td>\n",
       "      <td>0.030151</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.723315</td>\n",
       "      <td>0.030848</td>\n",
       "      <td>0.522523</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">0.5</th>\n",
       "      <th>1</th>\n",
       "      <td>0.736354</td>\n",
       "      <td>0.017677</td>\n",
       "      <td>0.509615</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.722708</td>\n",
       "      <td>0.035354</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.741535</td>\n",
       "      <td>0.030691</td>\n",
       "      <td>0.486239</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.740637</td>\n",
       "      <td>0.036269</td>\n",
       "      <td>0.482456</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.714763</td>\n",
       "      <td>0.038363</td>\n",
       "      <td>0.532110</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.751598</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.467391</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.724956</td>\n",
       "      <td>0.028721</td>\n",
       "      <td>0.521368</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.715089</td>\n",
       "      <td>0.059406</td>\n",
       "      <td>0.510417</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.753639</td>\n",
       "      <td>0.030457</td>\n",
       "      <td>0.462264</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.723658</td>\n",
       "      <td>0.027431</td>\n",
       "      <td>0.525253</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "metrics                                                        AUC       FNR  \\\n",
       "classifier  attack percentage_samples_poisoned iteration                       \n",
       "adaline     empty  0.0                         1          0.569565  0.000000   \n",
       "                                               2          0.580357  0.000000   \n",
       "                                               3          0.580189  0.000000   \n",
       "                                               4          0.567308  0.000000   \n",
       "                                               5          0.616391  0.002513   \n",
       "                                               6          0.576923  0.000000   \n",
       "                                               7          0.626316  0.000000   \n",
       "                                               8          0.601942  0.000000   \n",
       "                                               9          0.597222  0.000000   \n",
       "                                               10         0.566372  0.000000   \n",
       "                   0.1                         1          0.597222  0.000000   \n",
       "                                               2          0.549107  0.000000   \n",
       "                                               3          0.546296  0.000000   \n",
       "                                               4          0.564815  0.000000   \n",
       "                                               5          0.543269  0.000000   \n",
       "                                               6          0.556075  0.000000   \n",
       "                                               7          0.566667  0.000000   \n",
       "                                               8          0.577670  0.000000   \n",
       "                                               9          0.516667  0.000000   \n",
       "                                               10         0.551887  0.000000   \n",
       "                   0.2                         1          0.534783  0.000000   \n",
       "                                               2          0.542056  0.000000   \n",
       "                                               3          0.531532  0.000000   \n",
       "                                               4          0.516529  0.000000   \n",
       "                                               5          0.549020  0.000000   \n",
       "                                               6          0.545455  0.000000   \n",
       "                                               7          0.574257  0.000000   \n",
       "                                               8          0.569307  0.000000   \n",
       "                                               9          0.512605  0.000000   \n",
       "                                               10         0.542857  0.000000   \n",
       "...                                                            ...       ...   \n",
       "naive bayes ham    0.3                         1          0.739427  0.026764   \n",
       "                                               2          0.719352  0.032995   \n",
       "                                               3          0.709801  0.030848   \n",
       "                                               4          0.726535  0.055263   \n",
       "                                               5          0.755789  0.027638   \n",
       "                                               6          0.740919  0.027778   \n",
       "                                               7          0.731324  0.027919   \n",
       "                                               8          0.719860  0.051020   \n",
       "                                               9          0.734168  0.017903   \n",
       "                                               10         0.756259  0.023196   \n",
       "                   0.4                         1          0.716978  0.038265   \n",
       "                                               2          0.759985  0.017766   \n",
       "                                               3          0.750534  0.027778   \n",
       "                                               4          0.764667  0.012723   \n",
       "                                               5          0.731338  0.015584   \n",
       "                                               6          0.714474  0.021053   \n",
       "                                               7          0.759578  0.022901   \n",
       "                                               8          0.758681  0.044041   \n",
       "                                               9          0.739827  0.030151   \n",
       "                                               10         0.723315  0.030848   \n",
       "                   0.5                         1          0.736354  0.017677   \n",
       "                                               2          0.722708  0.035354   \n",
       "                                               3          0.741535  0.030691   \n",
       "                                               4          0.740637  0.036269   \n",
       "                                               5          0.714763  0.038363   \n",
       "                                               6          0.751598  0.029412   \n",
       "                                               7          0.724956  0.028721   \n",
       "                                               8          0.715089  0.059406   \n",
       "                                               9          0.753639  0.030457   \n",
       "                                               10         0.723658  0.027431   \n",
       "\n",
       "metrics                                                        FPR  \\\n",
       "classifier  attack percentage_samples_poisoned iteration             \n",
       "adaline     empty  0.0                         1          0.860870   \n",
       "                                               2          0.839286   \n",
       "                                               3          0.839623   \n",
       "                                               4          0.865385   \n",
       "                                               5          0.764706   \n",
       "                                               6          0.846154   \n",
       "                                               7          0.747368   \n",
       "                                               8          0.796117   \n",
       "                                               9          0.805556   \n",
       "                                               10         0.867257   \n",
       "                   0.1                         1          0.805556   \n",
       "                                               2          0.901786   \n",
       "                                               3          0.907407   \n",
       "                                               4          0.870370   \n",
       "                                               5          0.913462   \n",
       "                                               6          0.887850   \n",
       "                                               7          0.866667   \n",
       "                                               8          0.844660   \n",
       "                                               9          0.966667   \n",
       "                                               10         0.896226   \n",
       "                   0.2                         1          0.930435   \n",
       "                                               2          0.915888   \n",
       "                                               3          0.936937   \n",
       "                                               4          0.966942   \n",
       "                                               5          0.901961   \n",
       "                                               6          0.909091   \n",
       "                                               7          0.851485   \n",
       "                                               8          0.861386   \n",
       "                                               9          0.974790   \n",
       "                                               10         0.914286   \n",
       "...                                                            ...   \n",
       "naive bayes ham    0.3                         1          0.494382   \n",
       "                                               2          0.528302   \n",
       "                                               3          0.549550   \n",
       "                                               4          0.491667   \n",
       "                                               5          0.460784   \n",
       "                                               6          0.490385   \n",
       "                                               7          0.509434   \n",
       "                                               8          0.509259   \n",
       "                                               9          0.513761   \n",
       "                                               10         0.464286   \n",
       "                   0.4                         1          0.527778   \n",
       "                                               2          0.462264   \n",
       "                                               3          0.471154   \n",
       "                                               4          0.457944   \n",
       "                                               5          0.521739   \n",
       "                                               6          0.550000   \n",
       "                                               7          0.457944   \n",
       "                                               8          0.438596   \n",
       "                                               9          0.490196   \n",
       "                                               10         0.522523   \n",
       "                   0.5                         1          0.509615   \n",
       "                                               2          0.519231   \n",
       "                                               3          0.486239   \n",
       "                                               4          0.482456   \n",
       "                                               5          0.532110   \n",
       "                                               6          0.467391   \n",
       "                                               7          0.521368   \n",
       "                                               8          0.510417   \n",
       "                                               9          0.462264   \n",
       "                                               10         0.525253   \n",
       "\n",
       "metrics                                                   error_test  \\\n",
       "classifier  attack percentage_samples_poisoned iteration               \n",
       "adaline     empty  0.0                         1               0.198   \n",
       "                                               2               0.188   \n",
       "                                               3               0.178   \n",
       "                                               4               0.180   \n",
       "                                               5               0.158   \n",
       "                                               6               0.198   \n",
       "                                               7               0.142   \n",
       "                                               8               0.164   \n",
       "                                               9               0.174   \n",
       "                                               10              0.196   \n",
       "                   0.1                         1               0.174   \n",
       "                                               2               0.202   \n",
       "                                               3               0.196   \n",
       "                                               4               0.188   \n",
       "                                               5               0.190   \n",
       "                                               6               0.190   \n",
       "                                               7               0.182   \n",
       "                                               8               0.174   \n",
       "                                               9               0.232   \n",
       "                                               10              0.190   \n",
       "                   0.2                         1               0.214   \n",
       "                                               2               0.196   \n",
       "                                               3               0.208   \n",
       "                                               4               0.234   \n",
       "                                               5               0.184   \n",
       "                                               6               0.200   \n",
       "                                               7               0.172   \n",
       "                                               8               0.174   \n",
       "                                               9               0.232   \n",
       "                                               10              0.192   \n",
       "...                                                              ...   \n",
       "naive bayes ham    0.3                         1               0.110   \n",
       "                                               2               0.138   \n",
       "                                               3               0.146   \n",
       "                                               4               0.160   \n",
       "                                               5               0.116   \n",
       "                                               6               0.124   \n",
       "                                               7               0.130   \n",
       "                                               8               0.150   \n",
       "                                               9               0.126   \n",
       "                                               10              0.122   \n",
       "                   0.4                         1               0.144   \n",
       "                                               2               0.112   \n",
       "                                               3               0.120   \n",
       "                                               4               0.108   \n",
       "                                               5               0.132   \n",
       "                                               6               0.148   \n",
       "                                               7               0.116   \n",
       "                                               8               0.134   \n",
       "                                               9               0.124   \n",
       "                                               10              0.140   \n",
       "                   0.5                         1               0.120   \n",
       "                                               2               0.136   \n",
       "                                               3               0.130   \n",
       "                                               4               0.138   \n",
       "                                               5               0.146   \n",
       "                                               6               0.110   \n",
       "                                               7               0.144   \n",
       "                                               8               0.146   \n",
       "                                               9               0.122   \n",
       "                                               10              0.126   \n",
       "\n",
       "metrics                                                   error_train  \n",
       "classifier  attack percentage_samples_poisoned iteration               \n",
       "adaline     empty  0.0                         1                0.170  \n",
       "                                               2                0.170  \n",
       "                                               3                0.154  \n",
       "                                               4                0.184  \n",
       "                                               5                0.182  \n",
       "                                               6                0.168  \n",
       "                                               7                0.178  \n",
       "                                               8                0.166  \n",
       "                                               9                0.176  \n",
       "                                               10               0.184  \n",
       "                   0.1                         1                0.174  \n",
       "                                               2                0.170  \n",
       "                                               3                0.168  \n",
       "                                               4                0.168  \n",
       "                                               5                0.188  \n",
       "                                               6                0.154  \n",
       "                                               7                0.176  \n",
       "                                               8                0.172  \n",
       "                                               9                0.162  \n",
       "                                               10               0.186  \n",
       "                   0.2                         1                0.150  \n",
       "                                               2                0.160  \n",
       "                                               3                0.170  \n",
       "                                               4                0.152  \n",
       "                                               5                0.166  \n",
       "                                               6                0.162  \n",
       "                                               7                0.176  \n",
       "                                               8                0.150  \n",
       "                                               9                0.148  \n",
       "                                               10               0.168  \n",
       "...                                                               ...  \n",
       "naive bayes ham    0.3                         1                0.112  \n",
       "                                               2                0.086  \n",
       "                                               3                0.090  \n",
       "                                               4                0.072  \n",
       "                                               5                0.096  \n",
       "                                               6                0.098  \n",
       "                                               7                0.100  \n",
       "                                               8                0.082  \n",
       "                                               9                0.086  \n",
       "                                               10               0.082  \n",
       "                   0.4                         1                0.084  \n",
       "                                               2                0.104  \n",
       "                                               3                0.092  \n",
       "                                               4                0.094  \n",
       "                                               5                0.084  \n",
       "                                               6                0.066  \n",
       "                                               7                0.092  \n",
       "                                               8                0.072  \n",
       "                                               9                0.070  \n",
       "                                               10               0.074  \n",
       "                   0.5                         1                0.074  \n",
       "                                               2                0.082  \n",
       "                                               3                0.088  \n",
       "                                               4                0.066  \n",
       "                                               5                0.076  \n",
       "                                               6                0.076  \n",
       "                                               7                0.066  \n",
       "                                               8                0.080  \n",
       "                                               9                0.074  \n",
       "                                               10               0.076  \n",
       "\n",
       "[240 rows x 5 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = DF.reorder_levels(['classifier', 'attack', 'percentage_samples_poisoned', 'iteration']).sort_index()\n",
    "df.columns.names = ['metrics']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>metrics</th>\n",
       "      <th>AUC</th>\n",
       "      <th>FNR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>error_test</th>\n",
       "      <th>error_train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classifier</th>\n",
       "      <th>attack</th>\n",
       "      <th>percentage_samples_poisoned</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">adaline</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">empty</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.588258</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.823232</td>\n",
       "      <td>0.1776</td>\n",
       "      <td>0.1732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>0.556967</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.886065</td>\n",
       "      <td>0.1918</td>\n",
       "      <td>0.1718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>0.541840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.916320</td>\n",
       "      <td>0.2006</td>\n",
       "      <td>0.1602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.3</th>\n",
       "      <td>0.521371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.957257</td>\n",
       "      <td>0.2114</td>\n",
       "      <td>0.1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.4</th>\n",
       "      <td>0.516436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.967128</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.514543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.970914</td>\n",
       "      <td>0.2134</td>\n",
       "      <td>0.1304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">ham</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.583667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.832665</td>\n",
       "      <td>0.1872</td>\n",
       "      <td>0.1736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>0.559620</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.880759</td>\n",
       "      <td>0.1900</td>\n",
       "      <td>0.1632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>0.539232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.921536</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.3</th>\n",
       "      <td>0.523529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.952942</td>\n",
       "      <td>0.2072</td>\n",
       "      <td>0.1414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.4</th>\n",
       "      <td>0.520226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.959547</td>\n",
       "      <td>0.2068</td>\n",
       "      <td>0.1432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.510282</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.979437</td>\n",
       "      <td>0.2130</td>\n",
       "      <td>0.1264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">naive bayes</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">empty</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.740954</td>\n",
       "      <td>0.025474</td>\n",
       "      <td>0.492619</td>\n",
       "      <td>0.1266</td>\n",
       "      <td>0.1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>0.744276</td>\n",
       "      <td>0.028954</td>\n",
       "      <td>0.482494</td>\n",
       "      <td>0.1266</td>\n",
       "      <td>0.1124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>0.728110</td>\n",
       "      <td>0.025790</td>\n",
       "      <td>0.517990</td>\n",
       "      <td>0.1368</td>\n",
       "      <td>0.0910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.3</th>\n",
       "      <td>0.731545</td>\n",
       "      <td>0.027047</td>\n",
       "      <td>0.509863</td>\n",
       "      <td>0.1318</td>\n",
       "      <td>0.0860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.4</th>\n",
       "      <td>0.743858</td>\n",
       "      <td>0.026729</td>\n",
       "      <td>0.485555</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.0768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.729004</td>\n",
       "      <td>0.033987</td>\n",
       "      <td>0.508005</td>\n",
       "      <td>0.1398</td>\n",
       "      <td>0.0680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">ham</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.732018</td>\n",
       "      <td>0.024030</td>\n",
       "      <td>0.511935</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>0.1168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>0.735407</td>\n",
       "      <td>0.024467</td>\n",
       "      <td>0.504720</td>\n",
       "      <td>0.1286</td>\n",
       "      <td>0.1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>0.737030</td>\n",
       "      <td>0.029453</td>\n",
       "      <td>0.496487</td>\n",
       "      <td>0.1314</td>\n",
       "      <td>0.0944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.3</th>\n",
       "      <td>0.733343</td>\n",
       "      <td>0.032132</td>\n",
       "      <td>0.501181</td>\n",
       "      <td>0.1322</td>\n",
       "      <td>0.0904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.4</th>\n",
       "      <td>0.741938</td>\n",
       "      <td>0.026111</td>\n",
       "      <td>0.490014</td>\n",
       "      <td>0.1278</td>\n",
       "      <td>0.0832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.732494</td>\n",
       "      <td>0.033378</td>\n",
       "      <td>0.501634</td>\n",
       "      <td>0.1318</td>\n",
       "      <td>0.0758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "metrics                                              AUC       FNR       FPR  \\\n",
       "classifier  attack percentage_samples_poisoned                                 \n",
       "adaline     empty  0.0                          0.588258  0.000251  0.823232   \n",
       "                   0.1                          0.556967  0.000000  0.886065   \n",
       "                   0.2                          0.541840  0.000000  0.916320   \n",
       "                   0.3                          0.521371  0.000000  0.957257   \n",
       "                   0.4                          0.516436  0.000000  0.967128   \n",
       "                   0.5                          0.514543  0.000000  0.970914   \n",
       "            ham    0.0                          0.583667  0.000000  0.832665   \n",
       "                   0.1                          0.559620  0.000000  0.880759   \n",
       "                   0.2                          0.539232  0.000000  0.921536   \n",
       "                   0.3                          0.523529  0.000000  0.952942   \n",
       "                   0.4                          0.520226  0.000000  0.959547   \n",
       "                   0.5                          0.510282  0.000000  0.979437   \n",
       "naive bayes empty  0.0                          0.740954  0.025474  0.492619   \n",
       "                   0.1                          0.744276  0.028954  0.482494   \n",
       "                   0.2                          0.728110  0.025790  0.517990   \n",
       "                   0.3                          0.731545  0.027047  0.509863   \n",
       "                   0.4                          0.743858  0.026729  0.485555   \n",
       "                   0.5                          0.729004  0.033987  0.508005   \n",
       "            ham    0.0                          0.732018  0.024030  0.511935   \n",
       "                   0.1                          0.735407  0.024467  0.504720   \n",
       "                   0.2                          0.737030  0.029453  0.496487   \n",
       "                   0.3                          0.733343  0.032132  0.501181   \n",
       "                   0.4                          0.741938  0.026111  0.490014   \n",
       "                   0.5                          0.732494  0.033378  0.501634   \n",
       "\n",
       "metrics                                         error_test  error_train  \n",
       "classifier  attack percentage_samples_poisoned                           \n",
       "adaline     empty  0.0                              0.1776       0.1732  \n",
       "                   0.1                              0.1918       0.1718  \n",
       "                   0.2                              0.2006       0.1602  \n",
       "                   0.3                              0.2114       0.1500  \n",
       "                   0.4                              0.2100       0.1400  \n",
       "                   0.5                              0.2134       0.1304  \n",
       "            ham    0.0                              0.1872       0.1736  \n",
       "                   0.1                              0.1900       0.1632  \n",
       "                   0.2                              0.2028       0.1536  \n",
       "                   0.3                              0.2072       0.1414  \n",
       "                   0.4                              0.2068       0.1432  \n",
       "                   0.5                              0.2130       0.1264  \n",
       "naive bayes empty  0.0                              0.1266       0.1200  \n",
       "                   0.1                              0.1266       0.1124  \n",
       "                   0.2                              0.1368       0.0910  \n",
       "                   0.3                              0.1318       0.0860  \n",
       "                   0.4                              0.1270       0.0768  \n",
       "                   0.5                              0.1398       0.0680  \n",
       "            ham    0.0                              0.1310       0.1168  \n",
       "                   0.1                              0.1286       0.1080  \n",
       "                   0.2                              0.1314       0.0944  \n",
       "                   0.3                              0.1322       0.0904  \n",
       "                   0.4                              0.1278       0.0832  \n",
       "                   0.5                              0.1318       0.0758  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean = df.mean(level=df.index.names[:-1])\n",
    "df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier                    adaline           naive bayes          \n",
      "attack                          empty       ham       empty       ham\n",
      "percentage_samples_poisoned                                          \n",
      "0.0                          0.588258  0.583667    0.740954  0.732018\n",
      "0.1                          0.556967  0.559620    0.744276  0.735407\n",
      "0.2                          0.541840  0.539232    0.728110  0.737030\n",
      "0.3                          0.521371  0.523529    0.731545  0.733343\n",
      "0.4                          0.516436  0.520226    0.743858  0.741938\n",
      "0.5                          0.514543  0.510282    0.729004  0.732494\n",
      "{('adaline', 'empty'): 'r--',\n",
      " ('adaline', 'ham'): 'r-',\n",
      " ('naive bayes', 'empty'): 'g--',\n",
      " ('naive bayes', 'ham'): 'g-'}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6IAAAESCAYAAADnrWjbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3e4c529e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_plot = df_mean.unstack(['classifier', 'attack'])\n",
    "print(df_plot['AUC'])\n",
    "\n",
    "from itertools import product\n",
    "keys = list(product(*(list(names) for names in df_plot['AUC'].columns.levels)))\n",
    "style = {}\n",
    "for k in keys:\n",
    "    color = 'r' if 'adaline' in k else 'g'\n",
    "    ls = '-' if 'ham' in k else '--'\n",
    "    style[k] = '%s%s' % (color, ls)\n",
    "\n",
    "params = {\n",
    "    'style': style,\n",
    "    'ylim': (0, 1),\n",
    "    'legend': None,\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(df.columns), figsize=(16, 4))\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.1)\n",
    "\n",
    "for i, metric in enumerate(df.columns):\n",
    "    df_plot[metric].plot(ax=axes[i], title=metric, sharey=True, **params)\n",
    "    axes[i].set_xticklabels([])\n",
    "    axes[i].set_xlabel('')\n",
    "\n",
    "axes[0].set_xlabel('% of poisoned samples')\n",
    "axes[2].set_xticklabels(np.arange(0, 0.6, 0.1))\n",
    "axes[-1].legend(frameon=True)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
